from __future__ import annotations

import os
import time
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Tuple, Dict, Any, List
import logging

from celery import shared_task
from celery_once import QueueOnce

try:
    import tomllib  # Python 3.11+
except Exception:  # pragma: no cover
    tomllib = None  # type: ignore

# Import the pipeline runner
try:
    from run_as_pipeline import run_batch_pipeline
except Exception:  # pragma: no cover
    run_batch_pipeline = None  # type: ignore


LOCK_FILENAME = ".pg_analyzing.lock"
ERRORS_FILENAME = "ERRORS.txt"
WATCHER_LOG_FILENAME = "watcher.log"
RUN_SUMMARY_FILENAME = "run_summary.json"
PG_OUTPUT_DIRNAME = "PG output"

def _now() -> float:
    return time.time()


def _write_text(target_file: Path, content: str) -> None:
    target_file.write_text(content)


def _iter_all_files(root: Path) -> List[Path]:
    files: List[Path] = []
    for dirpath, _, filenames in os.walk(root):
        for name in filenames:
            if name in [WATCHER_LOG_FILENAME, LOCK_FILENAME]:
                continue
            files.append(Path(dirpath) / name)
    return files


def _latest_mtime_in_tree(root: Path) -> float:
    latest: float = 0.0
    for file_path in _iter_all_files(root):
        try:
            mtime = file_path.stat().st_mtime
            if mtime > latest:
                latest = mtime
        except FileNotFoundError:
            # File might be in flux; ignore
            continue
    return latest


def _tree_is_stable(root: Path, stable_seconds: int = 60) -> bool:
    if not root.exists():
        _debug(root, "Tree does not exist")
        return False
    latest_mtime = _latest_mtime_in_tree(root)
    _debug(root, f"latest_mtime: {latest_mtime}")
    return (_now() - latest_mtime) >= stable_seconds


def _select_pipeline_toml(dir_path: Path) -> Tuple[Optional[Path], Optional[str]]:
    # Case-insensitive discovery of .toml files
    tomls = sorted([p for p in dir_path.iterdir() if p.is_file() and p.name.lower().endswith(".toml")])
    tomls = [p for p in tomls if not p.name.lower().endswith('autogenerated_full_parameters.toml')]
    if len(tomls) == 0:
        return None, (
            "Pipeline toml file not found. Add a pipeline .toml file." 
        )
    if len(tomls) == 1:
        return tomls[0], None
    # Multiple: prefer one ending with pipeline.toml
    for t in tomls:
        if t.name.lower().endswith("pipeline.toml"):
            return t, None
    return None, (
        "Pipeline toml file not found. If there are multiple toml files in the directory, "
        "one of them MUST have a name ending in \"pipeline.toml\"."
    )


def _load_toml(toml_path: Path) -> Dict[str, Any]:
    if tomllib is None:
        raise RuntimeError("tomllib is unavailable in this Python environment")
    with open(toml_path, "rb") as fh:
        return tomllib.load(fh)


def _validate_pipeline_toml(toml_path: Path) -> Optional[str]:
    try:
        data = _load_toml(toml_path)
    except Exception as e:
        return f"Failed to parse TOML: {e}"

    # Required top-level keys
    for key in ("workflow", "data", "sample table"):
        if key not in data["general"]:
            return f"Required key missing in TOML: {key}. existing keys: {data["general"].keys()}. keys in data: {data.keys()}"

    # Validate data path
    data_path_value = data["general"]["data"]
    sample_table_value = data["general"]["sample table"]

    def _resolve_path(value: Any) -> Optional[Path]:
        if isinstance(value, str):
            p = (toml_path.parent / value).resolve()
            return p
        return None

    data_path = _resolve_path(data_path_value)
    sample_table_path = _resolve_path(sample_table_value)

    if data_path is None or not data_path.exists():
        return f"Data path does not exist: {data_path_value}"
    if sample_table_path is None or not sample_table_path.exists():
        return f"Sample table path does not exist: {sample_table_value}"

    return None


def _has_error_file(dir_path: Path) -> bool:
    return (dir_path / ERRORS_FILENAME).exists()


def _pg_output_dir(dir_path: Path) -> Path:
    return dir_path / PG_OUTPUT_DIRNAME


def _outside_files_latest_mtime(dir_path: Path, exclude: Path) -> float:
    latest: float = 0.0
    for dp, _, fns in os.walk(dir_path):
        current_dir = Path(dp)
        if exclude in current_dir.parents or current_dir == exclude:
            continue
        for fn in fns:
            fp = current_dir / fn
            try:
                mt = fp.stat().st_mtime
                if mt > latest:
                    latest = mt
            except FileNotFoundError:
                continue
    return latest


def _should_reanalyze(dir_path: Path) -> bool:
    # Condition 1: no ERROR.txt or ERRORS.txt present
    if _has_error_file(dir_path):
        return False
    output_dir = _pg_output_dir(dir_path)
    if not output_dir.exists():
        return True
    return False

def _is_currently_analyzing(dir_path: Path, max_age_seconds: int = 3600) -> bool:
    lock_path = dir_path / LOCK_FILENAME
    if not lock_path.exists():
        return False
    try:
        mtime = lock_path.stat().st_mtime
        if (_now() - mtime) > max_age_seconds:
            # Stale lock: clean up
            lock_path.unlink(missing_ok=True)  # type: ignore[arg-type]
            return False
    except Exception:
        # On any error checking the lock, assume stale and remove
        try:
            lock_path.unlink(missing_ok=True)  # type: ignore[arg-type]
        except Exception:
            pass
        return False
    return True


def _mark_analyzing(dir_path: Path) -> None:
    (dir_path / LOCK_FILENAME).write_text(datetime.now().isoformat())


def _clear_analyzing(dir_path: Path) -> None:
    try:
        (dir_path / LOCK_FILENAME).unlink(missing_ok=True)  # type: ignore[arg-type]
    except Exception:
        pass


def _safe_write_error(dir_path: Path, filename: str, message: str) -> None:
    try:
        target = dir_path / filename
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with target.open("a", encoding="utf-8") as fh:
            fh.write(f"[{ts}] {message}\n")
    except Exception:
        # Last resort: attempt to write to a timestamped error file (append semantics)
        ts_file = datetime.now().strftime("%Y%m%d_%H%M%S")
        with (dir_path / f"{filename}.{ts_file}").open("a", encoding="utf-8") as fh:
            fh.write(message + "\n")


def _append_watcher_log(dir_path: Path, message: str) -> None:
    try:
        log_file = dir_path / WATCHER_LOG_FILENAME
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with log_file.open("a", encoding="utf-8") as fh:
            fh.write(f"[{ts}] {message}\n")
    except Exception:
        # Best effort only
        pass


def _write_run_summary(dir_path: Path, summary: Dict[str, Any]) -> None:
    try:
        out_file = dir_path / RUN_SUMMARY_FILENAME
        import json
        out_file.write_text(json.dumps(summary, indent=2), encoding="utf-8")
    except Exception:
        # Best effort only
        pass


def _debug_enabled() -> bool:
    try:
        return os.environ.get("PG_WATCHER_DEBUG", "0") == "1"
    except Exception:
        return False


LOGGER = logging.getLogger(__name__)


def _debug(dir_path: Path, message: str) -> None:
    if _debug_enabled():
        _append_watcher_log(dir_path, f"DEBUG: {message}")
        try:
            LOGGER.debug("[watcher %s] %s", dir_path.name, message)
        except Exception:
            pass


def _launch_pipeline(dir_path: Path, toml_file: Path) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:
    if run_batch_pipeline is None:
        return "run_batch_pipeline is unavailable in this environment", None
    try:
        _append_watcher_log(dir_path, f"Launching pipeline with TOML: {toml_file.name}")
        result = run_batch_pipeline(str(toml_file))
        _append_watcher_log(dir_path, "Pipeline finished successfully")
        return None, result
    except Exception as e:
        tb = traceback.format_exc()
        _append_watcher_log(dir_path, f"Pipeline failed: {e}")
        
        # Check if this is a warning-related error and append a concise banner
        if "input warnings" in str(e):
            _safe_write_error(dir_path, ERRORS_FILENAME, f"{e}")
            _append_watcher_log(dir_path, f"Warnings written to {ERRORS_FILENAME}")
        
        return f"Pipeline execution failed: {e}\n\n{tb}", None


@shared_task(base=QueueOnce, once={"graceful": True})
def watch_pipeline_input(watch_directory: list[str]) -> None:
    watch_dir = Path(*watch_directory)
    if not watch_dir.exists() or not watch_dir.is_dir():
        return

    # Iterate immediate subdirectories of the watch directory
    for entry in sorted(watch_dir.iterdir()):
        if not entry.is_dir():
            continue

        _debug(entry, f"Considering directory: {entry.name}")

        # Skip if currently analyzing
        if _is_currently_analyzing(entry):
            _debug(entry, "Currently analyzing; skipping")
            continue

        # Determine whether to process (new or re-analyze conditions)
        output_dir = _pg_output_dir(entry)
        should_process: bool
        if output_dir.exists():
            should_process = _should_reanalyze(entry)
            reason = 'should reanalyze'
        else:
            should_process = True
        _debug(entry, f"should_process={should_process}")

        if not should_process:
            _debug(entry, "No processing needed; skipping")
            continue

        # If there is an error marker, skip per policy
        if _has_error_file(entry):
            _debug(entry, "Found error marker; skipping")
            continue

        # Ensure directory tree is stable (not being copied to)
        if not _tree_is_stable(entry, stable_seconds=60):
            _debug(entry, "Directory not stable; skipping for now")
            continue
        else:
            _debug(entry, "Directory stable; proceeding")

        # Resolve TOML selection
        toml_file, selection_error = _select_pipeline_toml(entry)
        if selection_error is not None or toml_file is None:
            _debug(entry, f"TOML selection error: {selection_error}")
            _safe_write_error(entry, ERRORS_FILENAME, selection_error or "Unknown TOML selection error")
            continue
        else:
            _debug(entry, f"Selected TOML: {toml_file.name}")

        # Validate TOML keys and paths
        validation_error = _validate_pipeline_toml(toml_file)
        if validation_error is not None:
            _debug(entry, f"Validation failed: {validation_error}")
            _safe_write_error(entry, ERRORS_FILENAME, validation_error)
            continue
        else:
            _debug(entry, "Validation passed")

        # Mark as analyzing to prevent duplicate runs
        _mark_analyzing(entry)
        _debug(entry, "Marked as analyzing")
        try:
            # Double-check stability right before launch
            if not _tree_is_stable(entry, stable_seconds=60):
                _clear_analyzing(entry)
                _debug(entry, "Unstable just before launch; cleared analyzing and skipping")
                continue

            # Launch the pipeline
            error_message, result = _launch_pipeline(entry, toml_file)
            if error_message:
                _safe_write_error(entry, ERRORS_FILENAME, error_message)
                _debug(entry, "Execution error written to ERRORS.txt")
            else:
                # Persist a lightweight run summary next to input folder
                output_dir = _pg_output_dir(entry)
                output_dir.mkdir(exist_ok=True)
                if isinstance(result, dict):
                    # Write into PG output directory for easier discovery
                    _write_run_summary(output_dir, result)
                    _debug(entry, "run_summary.json written")
        finally:
            _clear_analyzing(entry)
            _debug(entry, "Cleared analyzing lock")