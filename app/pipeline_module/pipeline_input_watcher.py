"""Directory watcher that runs the batch pipeline on stable input trees.

It selects a pipeline TOML under each subdirectory, validates it, ensures
the directory is quiescent, and invokes the pipeline runner. Results and
errors are written next to the inputs.
"""

from __future__ import annotations

import os
import time
import traceback
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional, Tuple, Dict, Any, List
import logging

from celery import shared_task
from celery_once import QueueOnce

try:
    import tomllib  # Python 3.11+
except Exception:  # pragma: no cover
    tomllib = None  # type: ignore

# Import the pipeline runner
try:
    from run_as_pipeline import run_batch_pipeline
except Exception:  # pragma: no cover
    run_batch_pipeline = None  # type: ignore


LOCK_FILENAME = ".pg_analyzing.lock"
ERRORS_FILENAME = "ERRORS.txt"
WATCHER_LOG_FILENAME = "watcher.log"
RUN_SUMMARY_FILENAME = "run_summary.json"
PG_OUTPUT_DIRNAME = "PG output"

def _now() -> float:
    """Return current time as seconds since epoch.

    :returns: Float seconds since epoch.
    """
    return time.time()


def _write_text(target_file: Path, content: str) -> None:
    """Write ``content`` to ``target_file`` using UTF-8 encoding.

    :param target_file: Destination file path.
    :param content: Text content to write.
    :returns: None.
    """
    target_file.write_text(content)


def _iter_all_files(root: Path) -> List[Path]:
    """Return a list of all file paths under a directory tree (excluding locks/logs).

    :param root: Root directory to walk.
    :returns: List of file paths.
    """
    files: List[Path] = []
    for dirpath, _, filenames in os.walk(root):
        for name in filenames:
            if name in [WATCHER_LOG_FILENAME, LOCK_FILENAME]:
                continue
            files.append(Path(dirpath) / name)
    return files


def _latest_mtime_in_tree(root: Path) -> float:
    """Return latest modification time for any file under ``root``.

    :param root: Root directory to scan.
    :returns: Latest modification time in seconds since epoch.
    """
    latest: float = 0.0
    for file_path in _iter_all_files(root):
        try:
            mtime = file_path.stat().st_mtime
            if mtime > latest:
                latest = mtime
        except FileNotFoundError:
            # File might be in flux; ignore
            continue
    return latest


def _tree_is_stable(root: Path, stable_seconds: int = 60) -> bool:
    """Check whether a directory tree is quiescent for ``stable_seconds``.

    :param root: Root directory to inspect.
    :param stable_seconds: Required inactivity window in seconds.
    :returns: True if stable, False otherwise.
    """
    if not root.exists():
        _debug(root, "Tree does not exist")
        return False
    latest_mtime = _latest_mtime_in_tree(root)
    _debug(root, f"latest_mtime: {latest_mtime}")
    return (_now() - latest_mtime) >= stable_seconds


def _select_pipeline_toml(dir_path: Path) -> Tuple[Optional[Path], Optional[str]]:
    """Select a single pipeline TOML file from a directory.

    Prefers a file ending with "pipeline.toml" if multiple are present.

    :param dir_path: Directory to search.
    :returns: Tuple of (selected_path or None, error_message or None).
    """
    # Case-insensitive discovery of .toml files
    tomls = sorted([p for p in dir_path.iterdir() if p.is_file() and p.name.lower().endswith(".toml")])
    tomls = [p for p in tomls if not p.name.lower().endswith('autogenerated_full_parameters.toml')]
    if len(tomls) == 0:
        return None, (
            "Pipeline toml file not found. Add a pipeline .toml file." 
        )
    if len(tomls) == 1:
        return tomls[0], None
    # Multiple: prefer one ending with pipeline.toml
    for t in tomls:
        if t.name.lower().endswith("pipeline.toml"):
            return t, None
    return None, (
        "Pipeline toml file not found. If there are multiple toml files in the directory, "
        "one of them MUST have a name ending in \"pipeline.toml\"."
    )


def _load_toml(toml_path: Path) -> Dict[str, Any]:
    """Load a TOML file to a dict using the stdlib tomllib.

    :param toml_path: Path to a TOML file.
    :returns: Parsed TOML document as a dict.
    :raises RuntimeError: If tomllib is unavailable.
    """
    if tomllib is None:
        raise RuntimeError("tomllib is unavailable in this Python environment")
    with open(toml_path, "rb") as fh:
        return tomllib.load(fh)


def _validate_pipeline_toml(toml_path: Path) -> Optional[str]:
    """Validate minimal required keys and paths in a pipeline TOML.

    :param toml_path: Path to the pipeline TOML.
    :returns: Error message string if invalid; otherwise None.
    """
    try:
        data = _load_toml(toml_path)
    except Exception as e:
        return f"Failed to parse TOML: {e}"

    # Required top-level keys
    for key in ("workflow", "data", "sample table"):
        if key not in data["general"]:
            return f"Required key missing in TOML: {key}. existing keys: {data["general"].keys()}. keys in data: {data.keys()}"

    # Validate data path
    data_path_value = data["general"]["data"]
    sample_table_value = data["general"]["sample table"]

    def _resolve_path(value: Any) -> Optional[Path]:
        if isinstance(value, str):
            p = (toml_path.parent / value).resolve()
            return p
        return None

    data_path = _resolve_path(data_path_value)
    sample_table_path = _resolve_path(sample_table_value)

    if data_path is None or not data_path.exists():
        return f"Data path does not exist: {data_path_value}"
    if sample_table_path is None or not sample_table_path.exists():
        return f"Sample table path does not exist: {sample_table_value}"

    return None


def _has_error_file(dir_path: Path) -> bool:
    """Return True if ERRORS.txt is present in a directory.

    :param dir_path: Directory to test.
    :returns: True if error marker exists.
    """
    return (dir_path / ERRORS_FILENAME).exists()

def _pg_output_dir(dir_path: Path) -> Path:
    """Return the expected PG output directory path for a run.

    :param dir_path: Base input directory.
    :returns: Output directory path.
    """
    return dir_path / PG_OUTPUT_DIRNAME

def _should_reanalyze(dir_path: Path) -> bool:
    """Return True if directory should be processed (new or missing outputs).

    :param dir_path: Candidate directory path.
    :returns: True if no error marker exists and PG output is missing.
    """
    # Condition 1: no ERROR.txt or ERRORS.txt present
    if _has_error_file(dir_path):
        return False
    output_dir = _pg_output_dir(dir_path)
    if not output_dir.exists():
        return True
    return False

def _is_currently_analyzing(dir_path: Path, max_age_seconds: int = 3600) -> bool:
    """Check if a directory holds a fresh analyzing lock file.

    :param dir_path: Directory to check.
    :param max_age_seconds: Consider lock stale if older than this window.
    :returns: True if analyzing lock exists and is fresh.
    """
    lock_path = dir_path / LOCK_FILENAME
    if not lock_path.exists():
        return False
    try:
        mtime = lock_path.stat().st_mtime
        if (_now() - mtime) > max_age_seconds:
            # Stale lock: clean up
            lock_path.unlink(missing_ok=True)  # type: ignore[arg-type]
            return False
    except Exception:
        # On any error checking the lock, assume stale and remove
        try:
            lock_path.unlink(missing_ok=True)  # type: ignore[arg-type]
        except Exception:
            pass
        return False
    return True


def _mark_analyzing(dir_path: Path) -> None:
    """Create/refresh the analyzing lock file for a directory.

    :param dir_path: Directory to mark.
    :returns: None.
    """
    (dir_path / LOCK_FILENAME).write_text(datetime.now().isoformat())


def _clear_analyzing(dir_path: Path) -> None:
    """Remove the analyzing lock file if present.

    :param dir_path: Directory to unmark.
    :returns: None.
    """
    try:
        (dir_path / LOCK_FILENAME).unlink(missing_ok=True)  # type: ignore[arg-type]
    except Exception:
        pass

def _safe_write_end_of_processing(dir_path: Path, status: str) -> None:
    """Write the end of processing status to a file.

    :param dir_path: Directory to write to.
    :param status: Status of the processing.
    :returns: None.
    """
    try:
        (dir_path / f'pipeline.{status}.txt').write_text(datetime.now().isoformat())
    except Exception:
        # Take a small break and try again in a different way - maybe something went wrong with the previous try or the file system.
        time.sleep(2)
        try:
            with open(dir_path / f'pipeline.{status}.txt','w',encoding='utf-8') as fh:
                fh.write(f'{status} at {datetime.now().isoformat()}')
        except Exception:
            # Give up. Something is seriously wrong.
            pass

def _safe_write_error(dir_path: Path, filename: str, message: str) -> None:
    """Append an error message to a file, with timestamp and fallback path.

    :param dir_path: Base directory foscript error file.
    :param filename: Error filename (e.g., 'ERRORS.txt').
    :param message: Error message to append.
    :returns: None.
    """
    try:
        target = dir_path / filename
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with target.open("a", encoding="utf-8") as fh:
            fh.write(f"[{ts}] {message}\n")
    except Exception:
        # Last resort: attempt to write to a timestamped error file (append semantics)
        ts_file = datetime.now().strftime("%Y%m%d_%H%M%S")
        with (dir_path / f"{filename}.{ts_file}").open("a", encoding="utf-8") as fh:
            fh.write(message + "\n")


def _append_watcher_log(dir_path: Path, message: str) -> None:
    """Append a timestamped line to the directory's watcher.log file.

    :param dir_path: Base directory for the log.
    :param message: Log message.
    :returns: None.
    """
    try:
        log_file = dir_path / WATCHER_LOG_FILENAME
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        with log_file.open("a", encoding="utf-8") as fh:
            fh.write(f"[{ts}] {message}\n")
    except Exception:
        # Best effort only
        pass


def _write_run_summary(dir_path: Path, summary: Dict[str, Any]) -> None:
    """Write a JSON run summary into the PG output directory.

    :param dir_path: Output directory (PG output).
    :param summary: Summary object to serialize as JSON.
    :returns: None.
    """
    try:
        out_file = dir_path / RUN_SUMMARY_FILENAME
        import json
        out_file.write_text(json.dumps(summary, indent=2), encoding="utf-8")
    except Exception:
        # Best effort only
        pass


def _debug_enabled() -> bool:
    """Return True if watcher debug logging is enabled via env var.

    :returns: True when PG_WATCHER_DEBUG=1.
    """
    try:
        return os.environ.get("PG_WATCHER_DEBUG", "0") == "1"
    except Exception:
        return False


LOGGER = logging.getLogger(__name__)


def _debug(dir_path: Path, message: str) -> None:
    """Write a debug message to both watcher.log and the logger when enabled.

    :param dir_path: Context directory for watcher.log.
    :param message: Debug message.
    :returns: None.
    """
    if _debug_enabled():
        _append_watcher_log(dir_path, f"DEBUG: {message}")
        try:
            LOGGER.debug("[watcher %s] %s", dir_path.name, message)
        except Exception:
            pass


def _launch_pipeline(dir_path: Path, toml_file: Path) -> Tuple[Optional[str], Optional[Dict[str, Any]]]:
    """Launch the pipeline and capture success or error information.

    :param dir_path: Input directory for logging and outputs.
    :param toml_file: Selected pipeline TOML path.
    :returns: Tuple of (error_message or None, result dict or None).
    """
    if run_batch_pipeline is None:
        return "run_batch_pipeline is unavailable in this environment", None
    try:
        _append_watcher_log(dir_path, f"Launching pipeline with TOML: {toml_file.name}")
        result = run_batch_pipeline(str(toml_file))
        _append_watcher_log(dir_path, "Pipeline finished successfully")
        return None, result
    except Exception as e:
        tb = traceback.format_exc()
        _append_watcher_log(dir_path, f"Pipeline failed: {e}")
        
        # Check if this is a warning-related error and append a concise banner
        if "input warnings" in str(e):
            _safe_write_error(dir_path, ERRORS_FILENAME, f"{e}")
            _append_watcher_log(dir_path, f"Warnings written to {ERRORS_FILENAME}")
        
        return f"Pipeline execution failed: {e}\n\n{tb}", None


@shared_task(base=QueueOnce, once={"graceful": True})
def watch_pipeline_input(watch_directory: list[str]) -> None:
    """Celery task that scans a watch directory and triggers pipeline runs.

    :param watch_directory: Directory path list (components) to monitor.
    :returns: None.
    """
    watch_dir = Path(*watch_directory)
    if not watch_dir.exists() or not watch_dir.is_dir():
        return

    # Iterate immediate subdirectories of the watch directory
    for entry in sorted(watch_dir.iterdir()):
        if not entry.is_dir():
            continue

        _debug(entry, f"Considering directory: {entry.name}")

        # Skip if currently analyzing
        if _is_currently_analyzing(entry):
            _debug(entry, "Currently analyzing; skipping")
            continue

        # Determine whether to process (new or re-analyze conditions)
        output_dir = _pg_output_dir(entry)
        should_process: bool
        if output_dir.exists():
            should_process = _should_reanalyze(entry)
            reason = 'should reanalyze'
        else:
            should_process = True
        _debug(entry, f"should_process={should_process}")

        if not should_process:
            _debug(entry, "No processing needed; skipping")
            continue

        # If there is an error marker, skip per policy
        if _has_error_file(entry):
            _debug(entry, "Found error marker; skipping")
            continue

        # Ensure directory tree is stable (not being copied to)
        if not _tree_is_stable(entry, stable_seconds=60):
            _debug(entry, "Directory not stable; skipping for now")
            continue
        else:
            _debug(entry, "Directory stable; proceeding")

        # Resolve TOML selection
        toml_file, selection_error = _select_pipeline_toml(entry)
        if selection_error is not None or toml_file is None:
            _debug(entry, f"TOML selection error: {selection_error}")
            _safe_write_error(entry, ERRORS_FILENAME, selection_error or "Unknown TOML selection error")
            continue
        else:
            _debug(entry, f"Selected TOML: {toml_file.name}")

        # Validate TOML keys and paths
        validation_error = _validate_pipeline_toml(toml_file)
        if validation_error is not None:
            _debug(entry, f"Validation failed: {validation_error}")
            _safe_write_error(entry, ERRORS_FILENAME, validation_error)
            continue
        else:
            _debug(entry, "Validation passed")

        # Mark as analyzing to prevent duplicate runs
        _mark_analyzing(entry)
        _debug(entry, "Marked as analyzing")
        try:
            # Double-check stability right before launch
            if not _tree_is_stable(entry, stable_seconds=60):
                _clear_analyzing(entry)
                _debug(entry, "Unstable just before launch; cleared analyzing and skipping")
                continue

            # Launch the pipeline
            error_message, result = _launch_pipeline(entry, toml_file)
            if error_message:
                _safe_write_error(entry, ERRORS_FILENAME, error_message)
                _safe_write_end_of_processing(entry, 'failure')
                _debug(entry, "Execution error written to ERRORS.txt")
            else:
                # Persist a lightweight run summary next to input folder
                output_dir = _pg_output_dir(entry)
                output_dir.mkdir(exist_ok=True)
                if isinstance(result, dict):
                    # Write into PG output directory for easier discovery
                    _write_run_summary(output_dir, result)
                    _debug(entry, "run_summary.json written")
                    _safe_write_end_of_processing(entry, 'success')
                else:
                    _safe_write_end_of_processing(entry, 'failure')
        finally:
            _clear_analyzing(entry)
            _debug(entry, "Cleared analyzing lock")