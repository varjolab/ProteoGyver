{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d962715f-5826-4962-a14c-dcd28a3e6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from dateutil.parser import parse\n",
    "from plotly import graph_objects as go\n",
    "from plotly import io as pio\n",
    "from components import text_handling\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52968b43-66fa-4b37-b568-f6271b4d1255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datadir = os.path.join('data_assets','db build files')\n",
    "dbdir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f708f09-59fb-41cc-a588-61fddd9d9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "crapome = pd.read_csv(os.path.join(datadir,'crapome table.tsv'),sep='\\t')\n",
    "controls = pd.read_csv(os.path.join(datadir,'control table.tsv'),sep='\\t')\n",
    "jsons = {}\n",
    "for f in os.listdir(datadir):\n",
    "    if f.split('.')[-1]=='json':\n",
    "        with open(os.path.join(datadir,f)) as fil:\n",
    "            jsons[f'data_{f}'] = json.load(fil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebdce825-9b88-43ad-9147-d33b1fb728ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = {\n",
    "    'VL GFP MAC3 AP': [\n",
    "             'VL GFP MAC3-N AP-MS',\n",
    "    ],\n",
    "    'VL GFP MAC3 BioID': [\n",
    "             'VL GFP MAC3-N BioID'\n",
    "    ],\n",
    "    'VL GFP MAC2 AP': [\n",
    "             'VL GFP MAC2-C AP-MS',\n",
    "             'VL GFP MAC2-N AP-MS',\n",
    "    ],\n",
    "    'VL GFP MAC2 BioID': [\n",
    "             'VL GFP MAC2-C BioID',\n",
    "             'VL GFP MAC2-N BioID'\n",
    "    ],\n",
    "    'VL GFP MAC AP': [\n",
    "            'VL GFP MAC-C AP-MS',\n",
    "            'VL GFP MAC-N AP-MS',\n",
    "    ],\n",
    "    'VL GFP MAC AP NLS': [\n",
    "            'VL GFP MAC-MED-NLS AP-MS',\n",
    "            'VL GFP MAC-MYC-NLS AP-MS',\n",
    "            'VL GFP MAC-NLS AP-MS',\n",
    "    ],\n",
    "    'VL GFP MAC BioID': [\n",
    "            'VL GFP MAC-C BioID',\n",
    "            'VL GFP MAC-N BioID'\n",
    "    ],\n",
    "    'VL GFP MAC BioID NLS': [\n",
    "            'VL GFP MAC-MED-NLS BioID',\n",
    "            'VL GFP MAC-MYC-NLS BioID',\n",
    "            'VL GFP MAC-NLS BioID'\n",
    "    ],\n",
    "    'Nesvilab': ['nesvilab']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea77eaf-21b4-436b-b44f-5ce10c21756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crapome_tables = {}\n",
    "columns = [\n",
    "    'protein_id',\n",
    "    'identified_in',\n",
    "    'frequency',\n",
    "    'spc_sum',\n",
    "    'spc_avg',\n",
    "    'spc_min',\n",
    "    'spc_max',\n",
    "    'spc_stdev']\n",
    "types = [\n",
    "    'TEXT PRIMARY KEY',\n",
    "    'INTEGER NOT NULL',\n",
    "    'REAL NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'REAL NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'REAL NOT NULL'\n",
    "]\n",
    "crapome_entries = []\n",
    "for setname, setcols in sets.items():\n",
    "    all_cols = ['PROTID']\n",
    "    defa = 1\n",
    "    if 'MAC2' in setname: defa = 0 # default enabled value\n",
    "    tablename = f'crapome_{setname}'.lower().replace(' ','_')\n",
    "    for sc in setcols:\n",
    "        all_cols.extend(jsons['data_crapome sets.json'][sc])\n",
    "    all_cols = sorted(list(set(all_cols)))\n",
    "    set_df = crapome[all_cols]\n",
    "    set_df.index = set_df['PROTID']\n",
    "    set_df = set_df.drop(columns=['PROTID']).replace(0,np.nan).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "    nruns = set_df.shape[1]\n",
    "    set_data = []\n",
    "    for protid, row in set_df.iterrows():\n",
    "        stdval = row.std()\n",
    "        if pd.isna(stdval):\n",
    "            stdval = -1\n",
    "        set_data.append([protid, row.notna().sum(), row.notna().sum()/nruns,row.sum(), row.mean(), row.min(), row.max(), stdval])\n",
    "    crapome_tables[tablename] = pd.DataFrame(columns=columns, data=set_data)\n",
    "    crapome_entries.append([tablename, setname, nruns, 0, defa, tablename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ae8a7be-795e-47b1-9c7d-6a10f105acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "control_tables = {}\n",
    "control_entries = []\n",
    "for setname, setcols in sets.items():\n",
    "    if setname == 'Nesvilab': continue\n",
    "    all_cols = ['PROTID']\n",
    "    defa = 1\n",
    "    if 'MAC2' in setname: defa = 0\n",
    "    tablename = f'control_{setname}'.lower().replace(' ','_')\n",
    "    for sc in setcols:\n",
    "        all_cols.extend(jsons['data_control sets.json'][sc])\n",
    "    all_cols = sorted(list(set(all_cols)))\n",
    "    set_df = controls[all_cols]\n",
    "    set_df.index = set_df['PROTID']\n",
    "    set_df = set_df.drop(columns=['PROTID']).replace(0,np.nan).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "    nruns = set_df.shape[1]\n",
    "    set_data = []\n",
    "    for protid, row in set_df.iterrows():\n",
    "        stdval = row.std()\n",
    "        if pd.isna(stdval):\n",
    "            stdval = -1\n",
    "        set_data.append([protid, row.notna().sum(), row.notna().sum()/nruns,row.sum(), row.mean(), row.min(), row.max(), stdval])\n",
    "    control_tables[tablename] = (set_df, pd.DataFrame(columns=columns, data=set_data))\n",
    "    control_entries.append([tablename, setname, nruns, 0, defa, tablename])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc12b865-b834-4dac-ae21-bb37c10e8ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control: 74150\n"
     ]
    }
   ],
   "source": [
    "control_cols = ['control_set','control_set_name','runs','is_disabled','is_default','control_table_name']\n",
    "crapome_cols = ['crapome_set','crapome_set_name','runs','is_disabled','is_default','crapome_table_name']\n",
    "exts = ['TEXT PRIMARY KEY','TEXT NOT NULL','INTEGER NOT NULL','INTEGER NOT NULL','INTEGER NOT NULL','TEXT NOT NULL']\n",
    "\n",
    "control_table_str =  [\n",
    "        f'CREATE TABLE control_sets (',\n",
    "    ]\n",
    "for i, c in enumerate(control_cols):\n",
    "    control_table_str.append(f'    {c} {exts[i]},',)\n",
    "control_table_str = '\\n'.join(control_table_str).strip(',')\n",
    "control_table_str += '\\n);'\n",
    "\n",
    "crapome_table_str =  [\n",
    "        f'CREATE TABLE crapome_sets (',\n",
    "    ]\n",
    "for i, c in enumerate(crapome_cols):\n",
    "    crapome_table_str.append(f'    {c} {exts[i]},',)\n",
    "crapome_table_str = '\\n'.join(crapome_table_str).strip(',')\n",
    "crapome_table_str += '\\n);'\n",
    "\n",
    "prot_cols = [\n",
    "    'uniprot_id',\n",
    "    'is_reviewed',\n",
    "    'gene_name',\n",
    "    'entry_name',\n",
    "    'all_gene_names',\n",
    "    'organism',\n",
    "    'length',\n",
    "    'sequence',\n",
    "    'is_latest',\n",
    "    'entry_source',\n",
    "    'update_time'\n",
    "]\n",
    "prot_exts = [\n",
    "    'TEXT PRIMARY KEY',\n",
    "    'INTEGER NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL'\n",
    "]\n",
    "\n",
    "prot_table_str =  [\n",
    "        f'CREATE TABLE proteins (',\n",
    "    ]\n",
    "for i, c in enumerate(prot_cols):\n",
    "    prot_table_str.append(f'    {c} {prot_exts[i]},',)\n",
    "prot_table_str = '\\n'.join(prot_table_str).strip(',')\n",
    "prot_table_str += '\\n);'\n",
    "\n",
    "table_create_sql = [control_table_str, crapome_table_str, prot_table_str]\n",
    "\n",
    "insert_sql = []\n",
    "\n",
    "for vals in control_entries:\n",
    "    tablename = vals[0]\n",
    "    detailed, overall = control_tables[tablename]\n",
    "    detailed.rename(\n",
    "        columns={\n",
    "            c: 'CS_'+text_handling.replace_accent_and_special_characters(c,'_')\n",
    "            for c in detailed.columns\n",
    "        },\n",
    "        inplace=True\n",
    "    )\n",
    "    create_str = [\n",
    "        f'CREATE TABLE {tablename}_overall (',\n",
    "    ]\n",
    "    for i, c in enumerate(overall.columns):\n",
    "        create_str.append(f'    {c} {types[i]},',)\n",
    "    create_str = '\\n'.join(create_str).strip(',')\n",
    "    create_str += '\\n);'\n",
    "    table_create_sql.append(create_str)\n",
    "    add_str = [f'INSERT INTO control_sets ({\", \".join(control_cols)}) VALUES ({\", \".join([\"?\" for _ in control_cols])})', vals]\n",
    "    insert_sql.append(add_str)\n",
    "    for _, row in overall.iterrows():\n",
    "        add_str = [f'INSERT INTO {tablename}_overall ({\", \".join(overall.columns)}) VALUES ({\", \".join([\"?\" for _ in overall.columns])})', tuple(row.values)]\n",
    "        insert_sql.append(add_str)\n",
    "    create_str = [\n",
    "        f'CREATE TABLE {tablename} (',\n",
    "    ]\n",
    "    detailed = detailed.reset_index()\n",
    "    detailed_control_types = ['TEXT PRIMARY KEY']\n",
    "    for c in detailed.columns[1:]:\n",
    "        detailed_control_types.append('REAL')\n",
    "    for i, c in enumerate(detailed.columns):\n",
    "        create_str.append(f'    {c} {detailed_control_types[i]},',)\n",
    "    create_str = '\\n'.join(create_str).strip(',')\n",
    "    create_str += '\\n);'\n",
    "    table_create_sql.append(create_str)\n",
    "    for _, row in detailed.iterrows():\n",
    "        add_str = [f'INSERT INTO {tablename} ({\", \".join(detailed.columns)}) VALUES ({\", \".join([\"?\" for _ in detailed.columns])})', tuple(row.values)]\n",
    "        insert_sql.append(add_str)\n",
    "print('control:', len(insert_sql))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae8cc049-662e-498a-8599-285f59246ab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crapome: 141853\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for vals in crapome_entries:\n",
    "    tablename = vals[0]\n",
    "    create_str = [\n",
    "        f'CREATE TABLE {tablename} (',\n",
    "    ]\n",
    "    for i, c in enumerate(columns):\n",
    "        create_str.append(f'    {c} {types[i]},',)\n",
    "    create_str = '\\n'.join(create_str).strip(',')\n",
    "    create_str += '\\n);'\n",
    "    table_create_sql.append(create_str)\n",
    "    add_str = [f'INSERT INTO crapome_sets ({\", \".join(crapome_cols)}) VALUES({\", \".join([\"?\" for _ in crapome_cols])})', vals]\n",
    "    insert_sql.append(add_str)\n",
    "    for _, row in crapome_tables[tablename].iterrows():\n",
    "        add_str = [f'INSERT INTO {tablename} ({\", \".join(columns)}) VALUES ({\", \".join([\"?\" for _ in columns])})', tuple(row.values)]\n",
    "        insert_sql.append(add_str)\n",
    "print('crapome:',len(insert_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "06d57b37-8909-489f-ac95-c88905fb9e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein: 228700\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "uniprots = set(pd.read_csv(os.path.join(datadir,'uniprotkb_AND_reviewed_true_2023_09_04.tsv'),sep='\\t')['Entry'].values)\n",
    "prot = pd.read_csv(os.path.join(datadir,'uniprotkb_taxonomy_id_7711_AND_reviewed_2023_09_04.tsv'),sep='\\t',index_col = 'Entry')\n",
    "prot = pd.read_csv(os.path.join(datadir,'uniprotkb_taxonomy_id_7711_AND_reviewed_2023_09_04.tsv'),sep='\\t',index_col = 'Entry')\n",
    "for protid, row in prot.iterrows():\n",
    "    gn = row['Gene Names (primary)']\n",
    "    if pd.isna(gn):\n",
    "        gn = row['Entry Name']\n",
    "    gns = row['Gene Names']\n",
    "    if pd.isna(gns):\n",
    "        gns = row['Entry Name']\n",
    "    row = row.fillna('')\n",
    "    data = [\n",
    "        protid,\n",
    "        int(row['Reviewed']=='reviewed'),\n",
    "        gn,\n",
    "        row['Entry Name'],\n",
    "        gns,\n",
    "        row['Organism'],\n",
    "        row['Length'],\n",
    "        row['Sequence'],\n",
    "        1,\n",
    "        'uniprot_initial_download',\n",
    "        datetime.today().strftime('%Y-%m-%d')\n",
    "    ]\n",
    "    add_str = f'INSERT INTO proteins ({\", \".join(prot_cols)}) VALUES ({\", \".join([\"?\" for _ in prot_cols])})'\n",
    "    insert_sql.append([add_str, data])\n",
    "print('protein:', len(insert_sql))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "150fb31f-f266-4220-85f2-2b4b2309423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cont_cols = [\n",
    "    'uniprot_id',\n",
    "    'is_reviewed',\n",
    "    'gene_name',\n",
    "    'entry_name',\n",
    "    'all_gene_names',\n",
    "    'organism',\n",
    "    'length',\n",
    "    'sequence',\n",
    "    'entry_source',\n",
    "    'contamination_source',\n",
    "    'update_time'\n",
    "]\n",
    "cont_exts = [\n",
    "    'TEXT PRIMARY KEY',\n",
    "    'INTEGER NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    \n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL'\n",
    "]\n",
    "\n",
    "conts = pd.read_csv(os.path.join(datadir,'contaminant_list.tsv'),sep='\\t')\n",
    "conts = conts[~conts['Uniprot ID'].isin(['P0C1U8','Q2FZL2'])]\n",
    "dd = pd.read_csv(os.path.join(datadir,'idmapping_2023_09_11.tsv'),sep='\\t')\n",
    "for _,row in dd.iterrows():\n",
    "    conts.loc[conts[conts['Uniprot ID']==row['Entry']].index,'Length'] = row['Length']\n",
    "dd2 = pd.read_csv(os.path.join(datadir,'idmapping_2023_09_121.tsv'),sep='\\t')\n",
    "for _, row in dd2.iterrows():\n",
    "    ctloc = conts[conts['Uniprot ID']==row['From']]\n",
    "    conts.loc[ctloc.index, 'Sequence'] = row['Sequence']\n",
    "    conts.loc[ctloc.index, 'Gene names'] = row['Gene Names']\n",
    "    conts.loc[ctloc.index, 'Length'] = row['Length']\n",
    "    conts.loc[ctloc.index, 'Status'] = row['Reviewed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95fd9d69-4d31-40a7-b468-830ff9f795f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "updf = pd.read_csv(os.path.join(datadir,'uniprotkb_AND_reviewed_true_2023_09_04.tsv'),sep='\\t')\n",
    "seqs = {row['Entry']: row['Sequence'] for _, row in updf.iterrows()}\n",
    "seq_col = []\n",
    "for _,row in conts.iterrows():\n",
    "    if row['Uniprot ID'] not in seqs:\n",
    "        seq_col.append('')\n",
    "    else:\n",
    "        seq_col.append(seqs[row['Uniprot ID']])\n",
    "conts['Sequence'] = seq_col\n",
    "conts['Length'] = conts['Length'].fillna(1).astype(int)\n",
    "for i, row in conts[conts['Gene names'].isna()].iterrows():\n",
    "    conts.loc[i, 'Gene names'] = f'{row[\"Protein names\"]}({row[\"Uniprot ID\"]})'\n",
    "conts['Organism'] = conts['Organism'].fillna('None')\n",
    "conts['Sequence'] = conts['Sequence'].fillna('Unknown')\n",
    "conts['Sequence'] = conts['Sequence'].fillna('Unknown')\n",
    "conts['Source of Contamination'] = conts['Source of Contamination'].fillna('Unspecified')\n",
    "cont_table_str =  [\n",
    "    f'CREATE TABLE contaminants (',\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f8da160-e360-416e-880b-d2f849263559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contaminants: 229147\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(cont_cols):\n",
    "    cont_table_str.append(f'    {c} {cont_exts[i]},',)\n",
    "cont_table_str = '\\n'.join(cont_table_str).strip(',')\n",
    "cont_table_str += '\\n);'\n",
    "for _, row in conts.iterrows():\n",
    "    gn = row['Gene names']\n",
    "    if not 'Uncharac' in gn:\n",
    "        gn = gn.split()[0]\n",
    "    gns = row['Gene names']\n",
    "    data = [\n",
    "        row['Uniprot ID'],\n",
    "        int(row['Status']=='reviewed'),\n",
    "        gn,\n",
    "        row['Entry name'],\n",
    "        gns,\n",
    "        row['Organism'],\n",
    "        row['Length'],\n",
    "        row['Sequence'],\n",
    "        row['DataBase'],\n",
    "        row['Source of Contamination'],\n",
    "        datetime.today().strftime('%Y-%m-%d')\n",
    "    ]\n",
    "    add_str = f'INSERT INTO contaminants ({\", \".join(cont_cols)}) VALUES ({\", \".join([\"?\" for _ in cont_cols])})'\n",
    "    insert_sql.append([add_str, data])\n",
    "table_create_sql.append(cont_table_str)\n",
    "print('contaminants:',len(insert_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5508bcd7-6134-4676-b4fa-3afbf9f3f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tictable_create = ['CREATE TABLE IF NOT EXISTS ms_runs (']\n",
    "tic_cols = [\n",
    "    'run_id TEXT PRIMARY KEY',\n",
    "    'sample_name TEXT NOT NULL',\n",
    "    'run_name TEXT NOT NULL',\n",
    "    'run_time TEXT NOT NULL',\n",
    "    'instrument TEXT NOT NULL',\n",
    "    'author TEXT NOT NULL',\n",
    "    'sample_type TEXT NOT NULL',\n",
    "    'run_type TEXT NOT NULL',\n",
    "    'lc_method TEXT NOT NULL',\n",
    "    'ms_method TEXT NOT NULL',\n",
    "    'bait TEXT',\n",
    "    'bait_uniprot TEXT',\n",
    "    'chromatogram_max_time INTEGER NOT NULL',\n",
    "    'cell_line TEXT',\n",
    "    'project TEXT',\n",
    "    'author_notes TEXT',\n",
    "    'bait_tag TEXT',\n",
    "]\n",
    "for col in tic_cols:\n",
    "    tictable_create.append(f'    {col},')\n",
    "for col in  [\n",
    "        'auc REAL NOT NULL',\n",
    "        'intercepts INTEGER NOT NULL',\n",
    "        'avg_peaks_per_timepoint REAL NOT NULL',\n",
    "        'mean_intensity INTEGER NOT NULL',\n",
    "        'max_intensity INTEGER NOT NULL',\n",
    "        'json TEXT NOT NULL',\n",
    "        'trace TEXT NOT NULL',\n",
    "    ]:\n",
    "    tic_cols.extend([\n",
    "        f'bpc_{col}',\n",
    "        f'msn_{col}',\n",
    "        f'tic_{col}'\n",
    "    ])\n",
    "    tictable_create.extend([f'    {c},' for c in tic_cols[-3:]])\n",
    "tictable_create = '\\n'.join(tictable_create).strip(',')\n",
    "tictable_create += '\\n);'\n",
    "table_create_sql.append(tictable_create)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c8963-ed4d-4207-a5d4-180bdb9dfe7f",
   "metadata": {},
   "source": [
    "## Run the dig_tic.py script at this point from an account with access to .d storage folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f056039-d5f7-4cc8-aa92-753db7294b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_name</th>\n",
       "      <th>run_id</th>\n",
       "      <th>run_name</th>\n",
       "      <th>lc_method</th>\n",
       "      <th>ms_method</th>\n",
       "      <th>run_time</th>\n",
       "      <th>chromatograms</th>\n",
       "      <th>chromatogram_max_time</th>\n",
       "      <th>bpc_auc</th>\n",
       "      <th>bpc_intercepts</th>\n",
       "      <th>...</th>\n",
       "      <th>tic_intercept_json</th>\n",
       "      <th>msn_auc</th>\n",
       "      <th>msn_intercepts</th>\n",
       "      <th>msn_avg_peaks_per_timepoint</th>\n",
       "      <th>msn_mean_intensity</th>\n",
       "      <th>msn_max_intensity</th>\n",
       "      <th>msn_json</th>\n",
       "      <th>msn_trace</th>\n",
       "      <th>msn_intercept_json</th>\n",
       "      <th>instrument</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C1</td>\n",
       "      <td>6135</td>\n",
       "      <td>C1_S1-D1_1_6135.d</td>\n",
       "      <td>Evosep_60SPD_21min.m</td>\n",
       "      <td>DDA PASEF-short_gradient_0.5sec_cycletime.m</td>\n",
       "      <td>2022-07-02_04:25:27</td>\n",
       "      <td>{\"json\": {\"BPC\": {\"0\": 13536.25, \"5\": 13346.0,...</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1.047688e+11</td>\n",
       "      <td>68.0</td>\n",
       "      <td>...</td>\n",
       "      <td>{\"300\": 22791259.689730536, \"435\": 22791259.68...</td>\n",
       "      <td>1.404762e+11</td>\n",
       "      <td>14.0</td>\n",
       "      <td>37.312253</td>\n",
       "      <td>724776.951454</td>\n",
       "      <td>1.417025e+06</td>\n",
       "      <td>{\"0\": 3973.3571428571427, \"5\": 3570.9393939393...</td>\n",
       "      <td>{\"name\":\"6135\",\"type\":\"scatter\",\"x\":[0,5,10,15...</td>\n",
       "      <td>{\"325\": 724776.951453663, \"330\": 724776.951453...</td>\n",
       "      <td>timsTOF pro 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C2</td>\n",
       "      <td>6136</td>\n",
       "      <td>C2_S1-D2_1_6136.d</td>\n",
       "      <td>Evosep_60SPD_21min.m</td>\n",
       "      <td>DDA PASEF-short_gradient_0.5sec_cycletime.m</td>\n",
       "      <td>2022-07-02_04:49:07</td>\n",
       "      <td>{\"json\": {\"BPC\": {\"0\": 13291.142857142857, \"5\"...</td>\n",
       "      <td>1260.0</td>\n",
       "      <td>1.000968e+11</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>{\"290\": 23355686.964923013, \"295\": 23355686.96...</td>\n",
       "      <td>1.479202e+11</td>\n",
       "      <td>18.0</td>\n",
       "      <td>37.063241</td>\n",
       "      <td>770903.047196</td>\n",
       "      <td>1.645680e+06</td>\n",
       "      <td>{\"0\": 3079.5, \"5\": 2809.3636363636365, \"10\": 3...</td>\n",
       "      <td>{\"name\":\"6136\",\"type\":\"scatter\",\"x\":[0,5,10,15...</td>\n",
       "      <td>{\"300\": 770903.047196216, \"305\": 770903.047196...</td>\n",
       "      <td>timsTOF pro 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_name  run_id           run_name             lc_method  \\\n",
       "0          C1    6135  C1_S1-D1_1_6135.d  Evosep_60SPD_21min.m   \n",
       "1          C2    6136  C2_S1-D2_1_6136.d  Evosep_60SPD_21min.m   \n",
       "\n",
       "                                     ms_method             run_time  \\\n",
       "0  DDA PASEF-short_gradient_0.5sec_cycletime.m  2022-07-02_04:25:27   \n",
       "1  DDA PASEF-short_gradient_0.5sec_cycletime.m  2022-07-02_04:49:07   \n",
       "\n",
       "                                       chromatograms  chromatogram_max_time  \\\n",
       "0  {\"json\": {\"BPC\": {\"0\": 13536.25, \"5\": 13346.0,...                 1260.0   \n",
       "1  {\"json\": {\"BPC\": {\"0\": 13291.142857142857, \"5\"...                 1260.0   \n",
       "\n",
       "        bpc_auc  bpc_intercepts  ...  \\\n",
       "0  1.047688e+11            68.0  ...   \n",
       "1  1.000968e+11            80.0  ...   \n",
       "\n",
       "                                  tic_intercept_json       msn_auc  \\\n",
       "0  {\"300\": 22791259.689730536, \"435\": 22791259.68...  1.404762e+11   \n",
       "1  {\"290\": 23355686.964923013, \"295\": 23355686.96...  1.479202e+11   \n",
       "\n",
       "   msn_intercepts msn_avg_peaks_per_timepoint msn_mean_intensity  \\\n",
       "0            14.0                   37.312253      724776.951454   \n",
       "1            18.0                   37.063241      770903.047196   \n",
       "\n",
       "  msn_max_intensity                                           msn_json  \\\n",
       "0      1.417025e+06  {\"0\": 3973.3571428571427, \"5\": 3570.9393939393...   \n",
       "1      1.645680e+06  {\"0\": 3079.5, \"5\": 2809.3636363636365, \"10\": 3...   \n",
       "\n",
       "                                           msn_trace  \\\n",
       "0  {\"name\":\"6135\",\"type\":\"scatter\",\"x\":[0,5,10,15...   \n",
       "1  {\"name\":\"6136\",\"type\":\"scatter\",\"x\":[0,5,10,15...   \n",
       "\n",
       "                                  msn_intercept_json     instrument  \n",
       "0  {\"325\": 724776.951453663, \"330\": 724776.951453...  timsTOF pro 2  \n",
       "1  {\"300\": 770903.047196216, \"305\": 770903.047196...  timsTOF pro 2  \n",
       "\n",
       "[2 rows x 33 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_cols = []\n",
    "for typ in ['BPC','TIC','MSn']:\n",
    "    for key in ['auc','intercepts','peaks_per_timepoint','mean_intensity','max_intensity', 'json','trace', 'intercept_json']:\n",
    "        new_cols.append(f'{typ}_{key}')\n",
    "col_renames = {c: c.replace('peaks_per_timepoint','avg_peaks_per_timepoint') for c in new_cols}\n",
    "tic_dir = os.path.join('data_assets','TIC data')\n",
    "tic_data_files = [os.path.join(tic_dir, f) for f in os.listdir(tic_dir) if 'TIC data_' in f]\n",
    "tic_data = pd.concat([pd.read_csv(f,sep='\\t') for f in tic_data_files]).drop(columns='Unnamed: 0')\n",
    "new_data = [[] for _ in new_cols]\n",
    "for _,row in tic_data.iterrows():\n",
    "    dic = json.loads(row['chromatograms'])\n",
    "    if len(dic['json'].keys()) == 0:\n",
    "        for i, nc in enumerate(new_cols):\n",
    "            new_data[i].append(np.nan)\n",
    "    else:\n",
    "        for i, nc in enumerate(new_cols):\n",
    "            typ, key = nc.split('_',maxsplit=1)\n",
    "            new_data[i].append(dic[key][typ])\n",
    "for i, nc in enumerate(new_cols):\n",
    "    tic_data[nc] = new_data[i]\n",
    "if not 'chromatogram_max_time' in tic_data.columns:\n",
    "    tvals = []\n",
    "    for _,row in tic_data.iterrows():\n",
    "        tvals.append(pd.Series(row['BPC_json']).index.max())\n",
    "    tic_data['chromatogram_max_time'] = tvals\n",
    "dic_cols = [c for c in tic_data.columns if 'json' in c]\n",
    "for d in dic_cols:\n",
    "    nvals = [json.dumps(v) if type(v) is dict else str(v).replace('nan','') for v in tic_data[d].values]\n",
    "    tic_data[d] = nvals\n",
    "tic_data = tic_data.rename(columns=col_renames)\n",
    "tic_data = tic_data.rename(columns={\n",
    "    c: c.lower() for c in tic_data.columns\n",
    "})\n",
    "tic_data = tic_data.rename(columns={\n",
    "    'sample_id': 'run_id', 'datafolder_name': 'run_name','HyStar_LC_Method_Name'.lower(): 'lc_method', 'HyStar_MS_Method_Name'.lower(): 'ms_method'\n",
    "})\n",
    "tic_data['instrument'] = 'timsTOF pro 2'\n",
    "tic_data['run_id'] = tic_data['run_id'].astype(int)\n",
    "tic_data.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0bbb3d9f-74cb-469e-a501-e633905f1a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "runlist = pd.read_excel(os.path.join('..','..','..','combined runlist.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "692a1ca1-2dfa-41bd-b623-aaa87502286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nvals = {c: [] for c in [r for r in tic_cols if r.split()[0] not in tic_data.columns]}\n",
    "for _,row in tic_data.iterrows():\n",
    "    run = runlist[runlist['Raw file']==row['run_name']]\n",
    "    if run.shape[0]==0:\n",
    "        for key in nvals.keys():\n",
    "            nvals[key].append('')\n",
    "    else:\n",
    "        run = run.iloc[0]\n",
    "        nvals['author TEXT NOT NULL'].append(run['Who'])\n",
    "        nvals['sample_type TEXT NOT NULL'].append(run['Sample type'])\n",
    "        nvals['run_type TEXT NOT NULL'].append(run['Run type'])\n",
    "        nvals['cell_line TEXT'].append(run['Cell line / material'])\n",
    "        nvals['project TEXT'].append(run['Project'])\n",
    "        nvals['author_notes TEXT'].append(run['Notes'])\n",
    "        nvals['bait_tag TEXT'].append(run['tag'])\n",
    "        nvals['bait TEXT'].append(run['Bait / other uniprot or ID'])\n",
    "        if run['Bait / other uniprot or ID'] in uniprots:\n",
    "            nvals['bait_uniprot TEXT'].append(run['Bait / other uniprot or ID'])\n",
    "        else:\n",
    "            nvals['bait_uniprot TEXT'].append('')\n",
    "for c, vals in nvals.items():\n",
    "    tic_data[c.split()[0]] = vals\n",
    "tic_data = tic_data[tic_data['chromatogram_max_time']!='']\n",
    "tic_data = tic_data[tic_data['chromatogram_max_time']!=0]\n",
    "tic_data = tic_data[tic_data['chromatogram_max_time'].notna()]\n",
    "tic_data = tic_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a2ada1f-5f79-465f-838a-363ae9501d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229147\n",
      "230997\n"
     ]
    }
   ],
   "source": [
    "print(len(insert_sql))\n",
    "for _,row in tic_data.iterrows():\n",
    "    data = [\n",
    "        row[c.split()[0]] for c in tic_cols\n",
    "    ]\n",
    "    add_str = f'INSERT INTO ms_runs ({\", \".join([c.split()[0] for c in tic_cols])}) VALUES ({\", \".join([\"?\" for _ in tic_cols])})'\n",
    "    insert_sql.append([add_str, data])\n",
    "print(len(insert_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66732d6e-9a58-49b1-963b-19c6272f8d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2022153/4071007100.py:33: DtypeWarning: Columns (20,21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  int_df = pd.read_csv(int_df_name,sep='\\t')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "intact.tsv 609399\n"
     ]
    }
   ],
   "source": [
    "inttable_create = ['CREATE TABLE IF NOT EXISTS known_interactions (']\n",
    "inttable_cols = [\n",
    "    'interaction TEXT PRIMARY KEY',\n",
    "    'uniprot_id_a TEXT NOT NULL',\n",
    "    'uniprot_id_b TEXT NOT NULL',\n",
    "    'uniprot_id_a_noiso TEXT NOT NULL',\n",
    "    'uniprot_id_b_noiso TEXT NOT NULL',\n",
    "    'source_database TEXT NOT NULL',\n",
    "    'isoform_a TEXT',\n",
    "    'isoform_b TEXT',\n",
    "    'experimental_role_interactor_a TEXT',\n",
    "    'interaction_detection_method TEXT',\n",
    "    'publication_identifier TEXT',\n",
    "    'biological_role_interactor_b TEXT',\n",
    "    'annotation_interactor_a TEXT',\n",
    "    'confidence_value TEXT',\n",
    "    'interaction_type TEXT',\n",
    "    'experimental_role_interactor_b TEXT',\n",
    "    'annotation_interactor_b TEXT',\n",
    "    'biological_role_interactor_a TEXT',\n",
    "    'notes TEXT',\n",
    "    'update_time TEXT',\n",
    "]\n",
    "for col in inttable_cols:\n",
    "    inttable_create.append(f'    {col},')\n",
    "inttable_create = '\\n'.join(inttable_create).strip(',')\n",
    "inttable_create += '\\n);'\n",
    "table_create_sql.append(inttable_create)\n",
    "print(len(insert_sql))\n",
    "\n",
    "\n",
    "for int_df_name in ['intact.tsv']:\n",
    "    int_df = pd.read_csv(int_df_name,sep='\\t')\n",
    "    int_df_slim = int_df[int_df['uniprot_id_a'].isin(prot.index.values) & int_df['uniprot_id_b'].isin(prot.index.values)]\n",
    "    for _,row in int_df_slim.iterrows():\n",
    "        data = [\n",
    "            row[c.split()[0]] for c in inttable_cols\n",
    "        ]\n",
    "        add_str = f'INSERT INTO known_interactions ({\", \".join([c.split()[0] for c in inttable_cols])}) VALUES ({\", \".join([\"?\" for _ in inttable_cols])})'\n",
    "        insert_sql.append([add_str, data])\n",
    "    print(int_df_name, len(insert_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67cac534-ae4c-48c6-9c2f-5bfd20b1f44e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609399\n"
     ]
    }
   ],
   "source": [
    "msmictable_create = ['CREATE TABLE IF NOT EXISTS msmicroscopy (']\n",
    "msmictable_cols = [\n",
    "    'Interaction TEXT PRIMARY KEY',\n",
    "    'Bait TEXT NOT NULL',\n",
    "    'Prey TEXT NOT NULL',\n",
    "    'Bait_norm REAL NOT NULL',\n",
    "    'Bait_sumnorm REAL NOT NULL',\n",
    "    'Loc TEXT NOT NULL',\n",
    "    'Unique_to_loc REAL NOT NULL',\n",
    "    'Loc_norm REAL NOT NULL',\n",
    "    'Loc_sumnorm REAL NOT NULL',\n",
    "    'MSMIC_version TEXT NOT NULL'\n",
    "]\n",
    "for col in msmictable_cols:\n",
    "    msmictable_create.append(f'    {col},')\n",
    "msmictable_create = '\\n'.join(msmictable_create).strip(',')\n",
    "msmictable_create += '\\n);'\n",
    "table_create_sql.append(msmictable_create)\n",
    "print(len(insert_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "250812cb-7b40-426f-bffa-cee18cd43432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.0 611002\n"
     ]
    }
   ],
   "source": [
    "for dirname in os.listdir(os.path.join(datadir,'msmic')):\n",
    "    if not os.path.isdir(os.path.join(datadir, 'msmic',dirname)):\n",
    "        continue\n",
    "    version = dirname\n",
    "    ref_data = pd.read_csv(os.path.join(datadir, 'msmic', version, 'msmic_ref_table.txt'),sep='\\t')\n",
    "    loc_data = pd.read_csv(os.path.join(datadir, 'msmic', version, 'msmic_localizations.txt'),sep='\\t')\n",
    "    loc_col = 'Organelle'\n",
    "\n",
    "    loc_data[loc_col] = [s.capitalize().strip() for s in loc_data[loc_col].values]\n",
    "    baitnorm = []\n",
    "    baitsumnorm = []\n",
    "    preys_in_baits = {}\n",
    "    preys_in_localizations = {}\n",
    "    db_bait_max = {}\n",
    "    db_bait_sum= {}\n",
    "    for b in ref_data['Bait'].unique():\n",
    "        db_bait_max[b] = max(ref_data[ref_data['Bait']==b]['AvgSpec'].values)\n",
    "        db_bait_sum[b] = sum(ref_data[ref_data['Bait']==b]['AvgSpec'].values)\n",
    "    for _,row in ref_data.iterrows():\n",
    "        if row['Prey'] not in preys_in_baits:\n",
    "            preys_in_baits[row['Prey']] = {}\n",
    "            preys_in_localizations[row['Prey']] = {}\n",
    "        preys_in_baits[row['Prey']][row['Bait']] = row['AvgSpec']\n",
    "        baitnorm.append(row['AvgSpec']/db_bait_max[row['Bait']])\n",
    "        baitsumnorm.append(row['AvgSpec']/db_bait_sum[row['Bait']])\n",
    "        localization = loc_data[loc_data['Bait']==row['Bait']].iloc[0][loc_col]\n",
    "        if localization not in preys_in_localizations:\n",
    "            preys_in_localizations[row['Prey']][localization] = []\n",
    "        preys_in_localizations[row['Prey']][localization].append(row['AvgSpec'])\n",
    "    ref_data['Bait_norm'] = baitnorm    \n",
    "    ref_data['Bait_sumnorm'] = baitsumnorm\n",
    "    unique_preys = [p for p, v in preys_in_localizations.items() if len(v) == 1]\n",
    "    ref_data['Loc'] = [loc_data[loc_data['Bait']==bait].iloc[0][loc_col] for bait in ref_data['Bait'].values]\n",
    "    ref_data['Unique_to_loc'] = [prey in unique_preys for prey in ref_data['Prey'].values]\n",
    "\n",
    "    uref = ref_data[ref_data['Unique_to_loc']].copy()\n",
    "    locnorm = []\n",
    "    locsumnorm = []\n",
    "    loc_max = {}\n",
    "    loc_sum = {}\n",
    "    for l in uref['Loc'].unique():\n",
    "        loc_max[l] = uref[uref['Loc']==l]['AvgSpec'].max()\n",
    "        loc_sum[l] = uref[uref['Loc']==l]['AvgSpec'].sum()\n",
    "    for _,row in uref.iterrows():\n",
    "        locnorm.append(row['AvgSpec']/loc_max[row['Loc']])\n",
    "        locsumnorm.append(row['AvgSpec']/loc_sum[row['Loc']])\n",
    "    uref['Loc_norm'] = locnorm\n",
    "    uref['Loc_sumnorm'] = locsumnorm\n",
    "    uref['MSMIC_version'] = version\n",
    "    uref['Interaction'] = uref['Bait']+uref['Prey']\n",
    "\n",
    "    for _,row in uref.iterrows():\n",
    "        data = [\n",
    "            row[c.split()[0]] for c in msmictable_cols\n",
    "        ]\n",
    "        add_str = f'INSERT INTO msmicroscopy ({\", \".join([c.split()[0] for c in msmictable_cols])}) VALUES ({\", \".join([\"?\" for _ in msmictable_cols])})'\n",
    "        insert_sql.append([add_str, data])\n",
    "    print(version, len(insert_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09838961-a88c-4c3b-b65b-d2fa5ac28893",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table creation and data insertion took 2 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Connect to the database (create it if it doesn't exist)\n",
    "conn = sqlite3.connect(os.path.join(dbdir,'proteogyver2.db'))\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "start = datetime.now()\n",
    "for create_table_str in table_create_sql:\n",
    "    cursor.execute(create_table_str)\n",
    "for insert_str, insert_data in insert_sql:\n",
    "    cursor.execute(insert_str, insert_data)\n",
    "print('Table creation and data insertion took', (datetime.now() - start).seconds, 'seconds')\n",
    "# Commit changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d305d6c-c730-4059-bffb-71475813fceb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WIP_inprogress",
   "language": "python",
   "name": "wip_inprogress"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
