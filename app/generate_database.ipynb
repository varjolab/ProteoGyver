{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d962715f-5826-4962-a14c-dcd28a3e6552",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from components.parsing import parse_parameters\n",
    "from dateutil.parser import parse\n",
    "from plotly import graph_objects as go\n",
    "from plotly import io as pio\n",
    "from components import text_handling\n",
    "import xmltodict\n",
    "from components.api_tools.annotation import intact\n",
    "from components.api_tools.annotation import biogrid\n",
    "parameters = parse_parameters('parameters.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f708f09-59fb-41cc-a588-61fddd9d9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbdir = os.path.join('data','db')\n",
    "datadir = os.path.join(dbdir,'db build files')\n",
    "crapome = pd.read_csv(os.path.join(datadir,'crapome table.tsv'),sep='\\t')\n",
    "controls = pd.read_csv(os.path.join(datadir,'control table.tsv'),sep='\\t')\n",
    "jsons = {}\n",
    "for f in os.listdir(datadir):\n",
    "    if f.split('.')[-1]=='json':\n",
    "        with open(os.path.join(datadir,f)) as fil:\n",
    "            jsons[f'data_{f}'] = json.load(fil)\n",
    "runlist = pd.read_excel(os.path.join('..','..','..','combined runlist.xlsx'))\n",
    "ms_run_datadir = '/run/user/1237916/gvfs/smb-share:server=biotek-filesrv1.ad.helsinki.fi,share=data1/varjosalo/Kari/202401 tims chroms/20240123 tics3/ms_runs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73bcab91-e080-4a03-bfb3-a69ed28d510d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sets = {\n",
    "    'VL GFP MAC3 10min AP': [\n",
    "        'VL GFP MAC3-N AP-MS',\n",
    "    ],\n",
    "    'VL GFP MAC3 10min BioID': [\n",
    "        'VL GFP MAC3-N BioID',\n",
    "    ],\n",
    "    'VL GFP MAC2 18h AP': [\n",
    "        'VL GFP MAC2-C AP-MS',\n",
    "        'VL GFP MAC2-N AP-MS',\n",
    "    ],\n",
    "    'VL GFP MAC2 18h BioID': [\n",
    "        'VL GFP MAC2-C BioID',\n",
    "        'VL GFP MAC2-N BioID'\n",
    "    ],\n",
    "    'VL GFP MAC 24h AP': [\n",
    "        'VL GFP MAC-C AP-MS',\n",
    "        'VL GFP MAC-N AP-MS',\n",
    "    ],\n",
    "    'VL GFP MAC 24h AP NLS': [\n",
    "        'VL GFP MAC-MED-NLS AP-MS',\n",
    "        'VL GFP MAC-MYC-NLS AP-MS',\n",
    "        'VL GFP MAC-NLS AP-MS',\n",
    "    ],\n",
    "    'VL GFP MAC 24h BioID': [\n",
    "        'VL GFP MAC-C BioID',\n",
    "        'VL GFP MAC-N BioID'\n",
    "    ],\n",
    "    'VL GFP MAC 24h BioID NLS': [\n",
    "        'VL GFP MAC-MED-NLS BioID',\n",
    "        'VL GFP MAC-MYC-NLS BioID',\n",
    "        'VL GFP MAC-NLS BioID'\n",
    "    ],\n",
    "    'Nesvilab': [\n",
    "        'nesvilab'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78c5e88c-e20e-40aa-840e-36ee623d0266",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "added 202401 Liu BioID\n",
      "added 202401 Liu AP-MS\n"
     ]
    }
   ],
   "source": [
    "additional_controls_dir = os.path.join(datadir,'gfp control')\n",
    "new_control_sets = {}\n",
    "overall_setnames = {\n",
    "    '202401 Liu AP-MS': 'VL GFP MAC3 10min AP',\n",
    "    '202401 Liu BioID': 'VL GFP MAC3 10min BioID',\n",
    "}\n",
    "for setdir in os.listdir(additional_controls_dir):\n",
    "    sampleinfo = pd.read_excel(os.path.join(additional_controls_dir, setdir, 'Sample_Information.xlsx'))\n",
    "    data = pd.read_csv(os.path.join(additional_controls_dir, '202401 Liu', 'reprint.spc.tsv'),sep='\\t', index_col='PROTID').drop(columns=['GENEID','PROTLEN'])\n",
    "    data = data[(data.index.notna()) & (~data.index.isin({'na','NA'}))].astype(int).replace('na',np.nan).replace(0, np.nan)\n",
    "    data = data.reset_index()\n",
    "    namecol, runidcol, samplename, _, expcol = sampleinfo.columns\n",
    "    for exp in sampleinfo[expcol].unique():\n",
    "        setname = f'{setdir} {exp}'\n",
    "        sets[overall_setnames[setname]].append(setname)\n",
    "        jsons['data_control sets.json'][setname] = list(data.columns)\n",
    "        jsons['data_crapome sets.json'][setname] = list(data.columns)\n",
    "        print(f'added {setname}')\n",
    "    crapome = crapome.merge(data,left_on='PROTID',right_on='PROTID',how='outer')\n",
    "    controls = controls.merge(data,left_on='PROTID',right_on='PROTID',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0ea8d5-c762-4165-aa71-c5eaabe6d2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1, 2}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([1,2,43])-set([43])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ea77eaf-21b4-436b-b44f-5ce10c21756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "crapome_tables = {}\n",
    "columns = [\n",
    "    'protein_id',\n",
    "    'identified_in',\n",
    "    'frequency',\n",
    "    'spc_sum',\n",
    "    'spc_avg',\n",
    "    'spc_min',\n",
    "    'spc_max',\n",
    "    'spc_stdev']\n",
    "types = [\n",
    "    'TEXT PRIMARY KEY',\n",
    "    'INTEGER NOT NULL',\n",
    "    'REAL NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'REAL NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'REAL NOT NULL'\n",
    "]\n",
    "crapome_entries = []\n",
    "for setname, setcols in sets.items():\n",
    "    all_cols = ['PROTID']\n",
    "    defa = 1\n",
    "    if 'MAC2' in setname: defa = 0 # default enabled value\n",
    "    tablename = f'crapome_{setname}'.lower().replace(' ','_')\n",
    "    for sc in setcols:\n",
    "        all_cols.extend(jsons['data_crapome sets.json'][sc])\n",
    "    all_cols = sorted(list(set(all_cols)))\n",
    "    set_df = crapome[all_cols]\n",
    "    setname = f'{setname} ({len(all_cols)} runs)'\n",
    "    set_df.index = set_df['PROTID']\n",
    "    set_df = set_df.drop(columns=['PROTID']).replace(0,np.nan).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "    nruns = set_df.shape[1]\n",
    "    set_data = []\n",
    "    for protid, row in set_df.iterrows():\n",
    "        stdval = row.std()\n",
    "        if pd.isna(stdval):\n",
    "            stdval = -1\n",
    "        set_data.append([protid, row.notna().sum(), row.notna().sum()/nruns,row.sum(), row.mean(), row.min(), row.max(), stdval])\n",
    "    crapome_tables[tablename] = pd.DataFrame(columns=columns, data=set_data)\n",
    "    crapome_entries.append([tablename, setname, nruns, 0, defa, tablename])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc12b865-b834-4dac-ae21-bb37c10e8ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "control: 83898\n",
      "crapome: 72577\n"
     ]
    }
   ],
   "source": [
    "control_tables = {}\n",
    "control_entries = []\n",
    "for setname, setcols in sets.items():\n",
    "    if setname == 'Nesvilab': continue\n",
    "    all_cols = ['PROTID']\n",
    "    defa = 1\n",
    "    if 'MAC2' in setname: defa = 0\n",
    "    tablename = f'control_{setname}'.lower().replace(' ','_')\n",
    "    for sc in setcols:\n",
    "        all_cols.extend(jsons['data_control sets.json'][sc])\n",
    "    all_cols = sorted(list(set(all_cols)))\n",
    "    setname = f'{setname} ({len(all_cols)} runs)'\n",
    "    set_df = controls[all_cols]\n",
    "    set_df.index = set_df['PROTID']\n",
    "    set_df = set_df.drop(columns=['PROTID']).replace(0,np.nan).dropna(how='all',axis=0).dropna(how='all',axis=1)\n",
    "    nruns = set_df.shape[1]\n",
    "    set_data = []\n",
    "    for protid, row in set_df.iterrows():\n",
    "        stdval = row.std()\n",
    "        if pd.isna(stdval):\n",
    "            stdval = -1\n",
    "        set_data.append([protid, row.notna().sum(), row.notna().sum()/nruns,row.sum(), row.mean(), row.min(), row.max(), stdval])\n",
    "    control_tables[tablename] = (set_df, pd.DataFrame(columns=columns, data=set_data))\n",
    "    control_entries.append([tablename, setname, nruns, 0, defa, tablename])\n",
    "\n",
    "control_cols = ['control_set','control_set_name','runs','is_disabled','is_default','control_table_name']\n",
    "crapome_cols = ['crapome_set','crapome_set_name','runs','is_disabled','is_default','crapome_table_name']\n",
    "exts = ['TEXT PRIMARY KEY','TEXT NOT NULL','INTEGER NOT NULL','INTEGER NOT NULL','INTEGER NOT NULL','TEXT NOT NULL']\n",
    "\n",
    "control_table_str =  [\n",
    "        f'CREATE TABLE IF NOT EXISTS control_sets (',\n",
    "    ]\n",
    "for i, c in enumerate(control_cols):\n",
    "    control_table_str.append(f'    {c} {exts[i]},',)\n",
    "control_table_str = '\\n'.join(control_table_str).strip(',')\n",
    "control_table_str += '\\n);'\n",
    "\n",
    "crapome_table_str =  [\n",
    "        f'CREATE TABLE IF NOT EXISTS  crapome_sets (',\n",
    "    ]\n",
    "for i, c in enumerate(crapome_cols):\n",
    "    crapome_table_str.append(f'    {c} {exts[i]},',)\n",
    "crapome_table_str = '\\n'.join(crapome_table_str).strip(',')\n",
    "crapome_table_str += '\\n);'\n",
    "\n",
    "prot_cols = [\n",
    "    'uniprot_id',\n",
    "    'is_reviewed',\n",
    "    'gene_name',\n",
    "    'entry_name',\n",
    "    'all_gene_names',\n",
    "    'organism',\n",
    "    'length',\n",
    "    'sequence',\n",
    "    'is_latest',\n",
    "    'entry_source',\n",
    "    'update_time'\n",
    "]\n",
    "prot_exts = [\n",
    "    'TEXT PRIMARY KEY',\n",
    "    'INTEGER NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL'\n",
    "]\n",
    "\n",
    "prot_table_str =  [\n",
    "        f'CREATE TABLE IF NOT EXISTS  proteins (',\n",
    "    ]\n",
    "for i, c in enumerate(prot_cols):\n",
    "    prot_table_str.append(f'    {c} {prot_exts[i]},',)\n",
    "prot_table_str = '\\n'.join(prot_table_str).strip(',')\n",
    "prot_table_str += '\\n);'\n",
    "\n",
    "table_create_sql = [control_table_str, crapome_table_str, prot_table_str]\n",
    "\n",
    "control_insert_sql = []\n",
    "for vals in control_entries:\n",
    "    tablename = vals[0]\n",
    "    detailed, overall = control_tables[tablename]\n",
    "    detailed.rename(\n",
    "        columns={\n",
    "            c: 'CS_'+text_handling.replace_accent_and_special_characters(c,'_')\n",
    "            for c in detailed.columns\n",
    "        },\n",
    "        inplace=True\n",
    "    )\n",
    "    create_str = [\n",
    "        f'CREATE TABLE IF NOT EXISTS  {tablename}_overall (',\n",
    "    ]\n",
    "    for i, c in enumerate(overall.columns):\n",
    "        create_str.append(f'    {c} {types[i]},',)\n",
    "    create_str = '\\n'.join(create_str).strip(',')\n",
    "    create_str += '\\n);'\n",
    "    table_create_sql.append(create_str)\n",
    "    add_str = [f'INSERT INTO control_sets ({\", \".join(control_cols)}) VALUES ({\", \".join([\"?\" for _ in control_cols])})', vals]\n",
    "    control_insert_sql.append(add_str)\n",
    "    for _, row in overall.iterrows():\n",
    "        add_str = [f'INSERT INTO {tablename}_overall ({\", \".join(overall.columns)}) VALUES ({\", \".join([\"?\" for _ in overall.columns])})', tuple(row.values)]\n",
    "        control_insert_sql.append(add_str)\n",
    "    create_str = [\n",
    "        f'CREATE TABLE IF NOT EXISTS  {tablename} (',\n",
    "    ]\n",
    "    detailed = detailed.reset_index()\n",
    "    detailed_control_types = ['TEXT PRIMARY KEY']\n",
    "    for c in detailed.columns[1:]:\n",
    "        detailed_control_types.append('REAL')\n",
    "    for i, c in enumerate(detailed.columns):\n",
    "        create_str.append(f'    {c} {detailed_control_types[i]},',)\n",
    "    create_str = '\\n'.join(create_str).strip(',')\n",
    "    create_str += '\\n);'\n",
    "    table_create_sql.append(create_str)\n",
    "    for _, row in detailed.iterrows():\n",
    "        add_str = [f'INSERT INTO {tablename} ({\", \".join(detailed.columns)}) VALUES ({\", \".join([\"?\" for _ in detailed.columns])})', tuple(row.values)]\n",
    "        control_insert_sql.append(add_str)\n",
    "print('control:', len(control_insert_sql))\n",
    "\n",
    "crapome_insert_sql = []\n",
    "for vals in crapome_entries:\n",
    "    tablename = vals[0]\n",
    "    create_str = [\n",
    "        f'CREATE TABLE IF NOT EXISTS  {tablename} (',\n",
    "    ]\n",
    "    for i, c in enumerate(columns):\n",
    "        create_str.append(f'    {c} {types[i]},',)\n",
    "    create_str = '\\n'.join(create_str).strip(',')\n",
    "    create_str += '\\n);'\n",
    "    table_create_sql.append(create_str)\n",
    "    add_str = [f'INSERT INTO crapome_sets ({\", \".join(crapome_cols)}) VALUES({\", \".join([\"?\" for _ in crapome_cols])})', vals]\n",
    "    crapome_insert_sql.append(add_str)\n",
    "    for _, row in crapome_tables[tablename].iterrows():\n",
    "        add_str = [f'INSERT INTO {tablename} ({\", \".join(columns)}) VALUES ({\", \".join([\"?\" for _ in columns])})', tuple(row.values)]\n",
    "        crapome_insert_sql.append(add_str)\n",
    "print('crapome:',len(crapome_insert_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d57b37-8909-489f-ac95-c88905fb9e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protein: 569793\n"
     ]
    }
   ],
   "source": [
    "uniprot_df = pd.read_csv(os.path.join(datadir,'uniprotkb_AND_reviewed_true_2023_09_04.tsv'),sep='\\t',index_col = 'Entry')\n",
    "uniprots = set(uniprot_df.index.values)\n",
    "proteins_insert_sql = []\n",
    "for protid, row in uniprot_df.iterrows():\n",
    "    gn = row['Gene Names (primary)']\n",
    "    if pd.isna(gn):\n",
    "        gn = row['Entry Name']\n",
    "    gns = row['Gene Names']\n",
    "    if pd.isna(gns):\n",
    "        gns = row['Entry Name']\n",
    "    row = row.fillna('')\n",
    "    data = [\n",
    "        protid,\n",
    "        int(row['Reviewed']=='reviewed'),\n",
    "        gn,\n",
    "        row['Entry Name'],\n",
    "        gns,\n",
    "        row['Organism'],\n",
    "        row['Length'],\n",
    "        row['Sequence'],\n",
    "        1,\n",
    "        'uniprot_initial_download',\n",
    "        datetime.today().strftime('%Y-%m-%d')\n",
    "    ]\n",
    "    add_str = f'INSERT INTO proteins ({\", \".join(prot_cols)}) VALUES ({\", \".join([\"?\" for _ in prot_cols])})'\n",
    "    proteins_insert_sql.append([add_str, data])\n",
    "print('protein:', len(proteins_insert_sql))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "150fb31f-f266-4220-85f2-2b4b2309423e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contaminants: 447\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cont_cols = [\n",
    "    'uniprot_id',\n",
    "    'is_reviewed',\n",
    "    'gene_name',\n",
    "    'entry_name',\n",
    "    'all_gene_names',\n",
    "    'organism',\n",
    "    'length',\n",
    "    'sequence',\n",
    "    'entry_source',\n",
    "    'contamination_source',\n",
    "    'update_time'\n",
    "]\n",
    "cont_exts = [\n",
    "    'TEXT PRIMARY KEY',\n",
    "    'INTEGER NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'INTEGER NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL',\n",
    "    'TEXT NOT NULL'\n",
    "]\n",
    "\n",
    "contaminants_insert_sql = []\n",
    "conts = pd.read_csv(os.path.join(datadir,'contaminant_list.tsv'),sep='\\t')\n",
    "conts = conts[~conts['Uniprot ID'].isin(['P0C1U8','Q2FZL2'])]\n",
    "dd = pd.read_csv(os.path.join(datadir,'idmapping_2023_09_11.tsv'),sep='\\t')\n",
    "for _,row in dd.iterrows():\n",
    "    conts.loc[conts[conts['Uniprot ID']==row['Entry']].index,'Length'] = row['Length']\n",
    "dd2 = pd.read_csv(os.path.join(datadir,'idmapping_2023_09_121.tsv'),sep='\\t')\n",
    "for _, row in dd2.iterrows():\n",
    "    ctloc = conts[conts['Uniprot ID']==row['From']]\n",
    "    conts.loc[ctloc.index, 'Sequence'] = row['Sequence']\n",
    "    conts.loc[ctloc.index, 'Gene names'] = row['Gene Names']\n",
    "    conts.loc[ctloc.index, 'Length'] = row['Length']\n",
    "    conts.loc[ctloc.index, 'Status'] = row['Reviewed']\n",
    "\n",
    "seqs = {entry: row['Sequence'] for entry, row in uniprot_df.iterrows()}\n",
    "seq_col = []\n",
    "for _,row in conts.iterrows():\n",
    "    if row['Uniprot ID'] not in seqs:\n",
    "        seq_col.append('')\n",
    "    else:\n",
    "        seq_col.append(seqs[row['Uniprot ID']])\n",
    "conts['Sequence'] = seq_col\n",
    "conts['Length'] = conts['Length'].fillna(1).astype(int)\n",
    "for i, row in conts[conts['Gene names'].isna()].iterrows():\n",
    "    conts.loc[i, 'Gene names'] = f'{row[\"Protein names\"]}({row[\"Uniprot ID\"]})'\n",
    "conts['Organism'] = conts['Organism'].fillna('None')\n",
    "conts['Sequence'] = conts['Sequence'].fillna('Unknown')\n",
    "conts['Sequence'] = conts['Sequence'].fillna('Unknown')\n",
    "conts['Source of Contamination'] = conts['Source of Contamination'].fillna('Unspecified')\n",
    "cont_table_str =  [\n",
    "    f'CREATE TABLE IF NOT EXISTS  contaminants (',\n",
    "]\n",
    "for i, c in enumerate(cont_cols):\n",
    "    cont_table_str.append(f'    {c} {cont_exts[i]},',)\n",
    "cont_table_str = '\\n'.join(cont_table_str).strip(',')\n",
    "cont_table_str += '\\n);'\n",
    "for _, row in conts.iterrows():\n",
    "    gn = row['Gene names']\n",
    "    if not 'Uncharac' in gn:\n",
    "        gn = gn.split()[0]\n",
    "    gns = row['Gene names']\n",
    "    data = [\n",
    "        row['Uniprot ID'],\n",
    "        int(row['Status']=='reviewed'),\n",
    "        gn,\n",
    "        row['Entry name'],\n",
    "        gns,\n",
    "        row['Organism'],\n",
    "        row['Length'],\n",
    "        row['Sequence'],\n",
    "        row['DataBase'],\n",
    "        row['Source of Contamination'],\n",
    "        datetime.today().strftime('%Y-%m-%d')\n",
    "    ]\n",
    "    add_str = f'INSERT INTO contaminants ({\", \".join(cont_cols)}) VALUES ({\", \".join([\"?\" for _ in cont_cols])})'\n",
    "    contaminants_insert_sql.append([add_str, data])\n",
    "table_create_sql.append(cont_table_str)\n",
    "print('contaminants:',len(contaminants_insert_sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43c8963-ed4d-4207-a5d4-180bdb9dfe7f",
   "metadata": {},
   "source": [
    "## Run the parse_tims_data.py script at this point from an account with access to .d storage folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a336261-1fb4-4106-9a65-07a4fee7df01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12582\n"
     ]
    }
   ],
   "source": [
    "mstable_create = ['CREATE TABLE IF NOT EXISTS ms_runs (']\n",
    "ms_cols = [\n",
    "    'run_id TEXT PRIMARY KEY',\n",
    "    'run_name TEXT NOT NULL',\n",
    "    'sample_name TEXT NOT NULL',\n",
    "    'file_name TEXT NOT NULL',\n",
    "    'run_time TEXT NOT NULL',\n",
    "    'run_date TEXT NOT NULL',\n",
    "    'instrument TEXT NOT NULL',\n",
    "    'author TEXT NOT NULL',\n",
    "    'sample_type TEXT NOT NULL',\n",
    "    'run_type TEXT NOT NULL',\n",
    "    'lc_method TEXT NOT NULL',\n",
    "    'ms_method TEXT NOT NULL',\n",
    "    'num_precursors INTEGER NOT NULL',\n",
    "    'bait TEXT',\n",
    "    'bait_uniprot TEXT',\n",
    "    'bait_mutation TEXT',\n",
    "    'chromatogram_max_time INTEGER NOT NULL',\n",
    "    'cell_line_or_material TEXT',\n",
    "    'project TEXT',\n",
    "    'author_notes TEXT',\n",
    "    'bait_tag TEXT'\n",
    "]\n",
    "keytypes = {\n",
    "    'auc': 'REAL NOT NULL',\n",
    "    'intercepts': 'INTEGER NOT NULL',\n",
    "    'avg_peaks_per_timepoint': 'REAL NOT NULL',\n",
    "    'mean_intensity': 'INTEGER NOT NULL',\n",
    "    'max_intensity': 'INTEGER NOT NULL',\n",
    "    'json': 'TEXT NOT NULL',\n",
    "    'trace': 'TEXT NOT NULL', \n",
    "    'intercept_json': 'TEXT NOT NULL'\n",
    "}\n",
    "for typ in ['MSn_filtered','TIC','MSn_unfiltered']:\n",
    "    for key in ['auc','intercepts','avg_peaks_per_timepoint','mean_intensity','max_intensity', 'json','trace', 'intercept_json']:\n",
    "        ms_cols.append(f'{typ.lower()}_{key.lower()} {keytypes[key]}')\n",
    "        \n",
    "ms_runs_insert_sql = []\n",
    "for col in ms_cols:\n",
    "    mstable_create.append(f'    {col},')\n",
    "mstable_create = '\\n'.join(mstable_create).strip(',')\n",
    "mstable_create += '\\n);'\n",
    "table_create_sql.append(mstable_create)\n",
    "data_to_enter = []\n",
    "failed_json_files = []\n",
    "runs_done = set()\n",
    "banned_run_dirs = [\n",
    "    'BRE_20_xxxxx_Helsinki',\n",
    "    'TrapTrouble_3'\n",
    "]\n",
    "for i, datafilename in enumerate(os.listdir(ms_run_datadir)):\n",
    "    with open(os.path.join(ms_run_datadir, datafilename)) as fil:\n",
    "        try:\n",
    "            dat = json.load(fil)\n",
    "        except json.JSONDecodeError:\n",
    "            failed_json_files.append(['json decode error', datafilename, ''])\n",
    "            continue\n",
    "    if dat['SampleID'] in runs_done: continue\n",
    "    if dat['SampleInfo'] == ['']: continue\n",
    "    banned = False\n",
    "    for b in banned_run_dirs:\n",
    "        if b in dat['SampleInfo']['SampleTable']['AnalysisHeader']['@FileName']:\n",
    "            banned = True\n",
    "    if banned:\n",
    "        continue\n",
    "    runs_done.add(dat['SampleID'])\n",
    "    lc_method = None\n",
    "    ms_method = None\n",
    "    if isinstance(dat['SampleInfo'], list):\n",
    "        failed_json_files.append(['no sample info',datafilename, dat])\n",
    "        continue\n",
    "    if not 'polarity_1' in dat:\n",
    "        failed_json_files.append(['no polarity',datafilename, dat])\n",
    "        continue\n",
    "    for propdic in dat['SampleInfo']['SampleTable']['SampleTableProperties']['Property']:\n",
    "        if propdic['@Name'] == 'HyStar_LC_Method_Name':\n",
    "            lc_method = propdic['@Value']\n",
    "        if propdic['@Name'] == 'HyStar_MS_Method_Name':\n",
    "            ms_method = propdic['@Value']\n",
    "    sample_names = {\n",
    "        dat['SampleInfo']['SampleTable']['Sample']['@SampleID'],\n",
    "        dat['SampleInfo']['SampleTable']['Sample']['@SampleID']+'.d',\n",
    "        dat['SampleInfo']['SampleTable']['Sample']['@DataPath'],\n",
    "    }\n",
    "    samplerow = runlist[runlist['Raw file'].isin(sample_names)]\n",
    "    if (lc_method is None) or (ms_method is None):\n",
    "        print('FAIL')\n",
    "        break\n",
    "    if len([k for k in dat.keys() if 'polarity' in k]) > 1:\n",
    "        print('FAIL2')\n",
    "        break\n",
    "    if samplerow.shape[0] == 0:\n",
    "        samplerow = pd.Series(index = samplerow.columns, data = ['No data' for c in samplerow.columns])\n",
    "    else:\n",
    "        samplerow = samplerow.iloc[0]\n",
    "    instrument = 'TimsTOF 1'\n",
    "    frame_df_name = f'{instrument} {dat[\"SampleID\"]}'\n",
    "    frame_df = pd.read_json(json.dumps(dat['Frames']),orient='split')\n",
    "    runtime = datetime.strftime(\n",
    "        datetime.strptime(\n",
    "            dat['SampleInfo']['SampleTable']['AnalysisHeader']['@CreationDateTime'].split('+')[0],\n",
    "            '%Y-%m-%dT%H:%M:%S'\n",
    "        ),\n",
    "        parameters['Config']['Time format']\n",
    "    )\n",
    "        \n",
    "    \n",
    "    samplename = samplerow['Sample name']\n",
    "    author = samplerow['Who']\n",
    "    sample_type = samplerow['Sample type']\n",
    "    bait = samplerow['Bait name']\n",
    "    bait_uniprot = samplerow['Bait / other uniprot or ID']\n",
    "    bait_mut = samplerow['Bait mutation']\n",
    "    cell_line = samplerow['Cell line / material']\n",
    "    project = samplerow['Project']\n",
    "    author_notes = samplerow['Notes']\n",
    "    bait_tag = samplerow['tag']\n",
    "    try:\n",
    "        precur = dat['NumPrecursors']\n",
    "    except KeyError:\n",
    "        precur = 'No precursor data'\n",
    "    ms_run_row = [\n",
    "        dat['SampleID'],\n",
    "        dat['SampleInfo']['SampleTable']['AnalysisHeader']['@SampleID'],\n",
    "        samplename,\n",
    "        dat['SampleInfo']['SampleTable']['AnalysisHeader']['@FileName'],\n",
    "        runtime,\n",
    "        runtime.split()[0],\n",
    "        instrument,\n",
    "        author,\n",
    "        sample_type,\n",
    "        dat['DataType'],\n",
    "        lc_method,\n",
    "        ms_method,\n",
    "        precur,\n",
    "        bait,\n",
    "        bait_uniprot,\n",
    "        bait_mut,\n",
    "        len(pd.Series(dat['polarity_1']['tic df']['Series'])),\n",
    "        cell_line,\n",
    "        project,\n",
    "        author_notes,\n",
    "        bait_tag\n",
    "    ]\n",
    "    for dataname in ['bpc filtered df', 'tic df', 'bpc unfiltered df']:\n",
    "        ms_run_row.extend([\n",
    "            dat['polarity_1'][dataname]['auc'],\n",
    "            dat['polarity_1'][dataname]['intercepts'],\n",
    "            dat['polarity_1'][dataname]['peaks_per_timepoint'],\n",
    "            dat['polarity_1'][dataname]['mean_intensity'],\n",
    "            dat['polarity_1'][dataname]['max_intensity'],\n",
    "            json.dumps(dat['polarity_1'][dataname]['Series']),\n",
    "            dat['polarity_1'][dataname]['trace'],\n",
    "            json.dumps(dat['polarity_1'][dataname]['intercept_dict']),\n",
    "        ])   \n",
    "    \n",
    "    data_to_enter.append(ms_run_row)\n",
    "for data in data_to_enter:\n",
    "    add_str = f'INSERT INTO ms_runs ({\", \".join([c.split()[0] for c in ms_cols])}) VALUES ({\", \".join([\"?\" for _ in ms_cols])})'\n",
    "    ms_runs_insert_sql.append([add_str, data])\n",
    "print(len(ms_runs_insert_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66732d6e-9a58-49b1-963b-19c6272f8d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowns 2500023\n"
     ]
    }
   ],
   "source": [
    "inttable_create = ['CREATE TABLE IF NOT EXISTS known_interactions (']\n",
    "inttable_cols = [\n",
    "    'interaction TEXT PRIMARY KEY',\n",
    "    'uniprot_id_a TEXT NOT NULL',\n",
    "    'uniprot_id_b TEXT NOT NULL',\n",
    "    'uniprot_id_a_noiso TEXT NOT NULL',\n",
    "    'uniprot_id_b_noiso TEXT NOT NULL',\n",
    "    'source_database TEXT NOT NULL',\n",
    "    'isoform_a TEXT',\n",
    "    'isoform_b TEXT',\n",
    "    'experimental_role_interactor_a TEXT',\n",
    "    'interaction_detection_method TEXT',\n",
    "    'publication_identifier TEXT',\n",
    "    'biological_role_interactor_b TEXT',\n",
    "    'annotation_interactor_a TEXT',\n",
    "    'confidence_value TEXT',\n",
    "    'interaction_type TEXT',\n",
    "    'experimental_role_interactor_b TEXT',\n",
    "    'annotation_interactor_b TEXT',\n",
    "    'biological_role_interactor_a TEXT',\n",
    "    'publication_count TEXT',\n",
    "    'notes TEXT',\n",
    "    'update_time TEXT',\n",
    "]\n",
    "for col in inttable_cols:\n",
    "    inttable_create.append(f'    {col},')\n",
    "inttable_create = '\\n'.join(inttable_create).strip(',')\n",
    "inttable_create += '\\n);'\n",
    "table_create_sql.append(inttable_create)\n",
    "known_interactions_insert_sql = []\n",
    "\n",
    "biogrid.update(uniprots)\n",
    "intact.update(uniprots)\n",
    "dbtables = [intact.get_latest(), biogrid.get_latest()]\n",
    "for d in dbtables:\n",
    "    if 'Unnamed: 0' in d.columns:\n",
    "        d.drop(columns=['Unnamed: 0'],inplace = True)\n",
    "shared = set()\n",
    "for i, d in enumerate(dbtables):\n",
    "    for d2 in dbtables[i+1:]:\n",
    "        shared |= (set(d2.index) & set(d.index))\n",
    "\n",
    "shared = sorted(list(shared))\n",
    "ind = 0\n",
    "if len(shared) > 0:\n",
    "    mtables = [d.loc[shared].sort_index() for d in dbtables]\n",
    "    dbtables = [d.drop(index=shared) for d in dbtables]\n",
    "new_data = []\n",
    "no_set = [c for c in mtables[0].columns if (('uniprot' not in c))]\n",
    "for c in mtables[0].columns:\n",
    "    if c == 'source_database':\n",
    "        jst = ';'\n",
    "    else:\n",
    "        jst = '__'\n",
    "    if c in no_set:\n",
    "        nc = mtables[0][c].astype(str)\n",
    "        for d in mtables[1:]:\n",
    "            nc = nc + jst + d[c].astype(str)\n",
    "        new_data.append(nc)\n",
    "    else:\n",
    "        new_data.append(mtables[0][c])\n",
    "newer_data = [\n",
    "    c.str.replace('nan','').str.strip('_') for c in new_data\n",
    "]\n",
    "shared_df = pd.DataFrame.from_dict({c: newer_data[i] for i, c in enumerate(mtables[0].columns)}).replace('__',np.nan)\n",
    "shared_df.index = mtables[0].index\n",
    "dbtables.append(shared_df)\n",
    "merg = pd.concat(dbtables)\n",
    "pm = []\n",
    "npubs = []\n",
    "for _,row in merg.iterrows():\n",
    "    pmids = set()\n",
    "    for p in row['publication_identifier'].split('__'):\n",
    "        for pp in p.split(';'):\n",
    "            if 'pubmed' in pp.lower():\n",
    "                pmids.add(pp)\n",
    "    pm.append(len(pmids))\n",
    "    npubs.append(';'.join(sorted(list(pmids))))\n",
    "merg['publication_count'] = pm\n",
    "merg['publication_identifier'] = npubs\n",
    "\n",
    "int_df_slim = merg[merg['uniprot_id_a'].isin(uniprots) & merg['uniprot_id_b'].isin(uniprots)]\n",
    "int_df_slim = int_df_slim.reset_index()\n",
    "for _,row in int_df_slim.iterrows():\n",
    "    data = [\n",
    "        row[c.split()[0]] for c in inttable_cols\n",
    "    ]\n",
    "    add_str = f'INSERT INTO known_interactions ({\", \".join([c.split()[0] for c in inttable_cols])}) VALUES ({\", \".join([\"?\" for _ in inttable_cols])})'\n",
    "    known_interactions_insert_sql.append([add_str, data])\n",
    "print('Knowns', len(known_interactions_insert_sql))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "250812cb-7b40-426f-bffa-cee18cd43432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.0 1603\n"
     ]
    }
   ],
   "source": [
    "msmictable_create = ['CREATE TABLE IF NOT EXISTS msmicroscopy (']\n",
    "msmictable_cols = [\n",
    "    'Interaction TEXT PRIMARY KEY',\n",
    "    'Bait TEXT NOT NULL',\n",
    "    'Prey TEXT NOT NULL',\n",
    "    'Bait_norm REAL NOT NULL',\n",
    "    'Bait_sumnorm REAL NOT NULL',\n",
    "    'Loc TEXT NOT NULL',\n",
    "    'Unique_to_loc REAL NOT NULL',\n",
    "    'Loc_norm REAL NOT NULL',\n",
    "    'Loc_sumnorm REAL NOT NULL',\n",
    "    'MSMIC_version TEXT NOT NULL'\n",
    "]\n",
    "for col in msmictable_cols:\n",
    "    msmictable_create.append(f'    {col},')\n",
    "msmictable_create = '\\n'.join(msmictable_create).strip(',')\n",
    "msmictable_create += '\\n);'\n",
    "msmicroscopy_insert_sql = []\n",
    "table_create_sql.append(msmictable_create)\n",
    "for dirname in os.listdir(os.path.join(datadir,'msmic')):\n",
    "    if not os.path.isdir(os.path.join(datadir, 'msmic',dirname)):\n",
    "        continue\n",
    "    version = dirname\n",
    "    ref_data = pd.read_csv(os.path.join(datadir, 'msmic', version, 'msmic_ref_table.txt'),sep='\\t')\n",
    "    loc_data = pd.read_csv(os.path.join(datadir, 'msmic', version, 'msmic_localizations.txt'),sep='\\t')\n",
    "    loc_col = 'Organelle'\n",
    "\n",
    "    loc_data[loc_col] = [s.capitalize().strip() for s in loc_data[loc_col].values]\n",
    "    baitnorm = []\n",
    "    baitsumnorm = []\n",
    "    preys_in_baits = {}\n",
    "    preys_in_localizations = {}\n",
    "    db_bait_max = {}\n",
    "    db_bait_sum= {}\n",
    "    for b in ref_data['Bait'].unique():\n",
    "        db_bait_max[b] = max(ref_data[ref_data['Bait']==b]['AvgSpec'].values)\n",
    "        db_bait_sum[b] = sum(ref_data[ref_data['Bait']==b]['AvgSpec'].values)\n",
    "    for _,row in ref_data.iterrows():\n",
    "        if row['Prey'] not in preys_in_baits:\n",
    "            preys_in_baits[row['Prey']] = {}\n",
    "            preys_in_localizations[row['Prey']] = {}\n",
    "        preys_in_baits[row['Prey']][row['Bait']] = row['AvgSpec']\n",
    "        baitnorm.append(row['AvgSpec']/db_bait_max[row['Bait']])\n",
    "        baitsumnorm.append(row['AvgSpec']/db_bait_sum[row['Bait']])\n",
    "        localization = loc_data[loc_data['Bait']==row['Bait']].iloc[0][loc_col]\n",
    "        if localization not in preys_in_localizations:\n",
    "            preys_in_localizations[row['Prey']][localization] = []\n",
    "        preys_in_localizations[row['Prey']][localization].append(row['AvgSpec'])\n",
    "    ref_data['Bait_norm'] = baitnorm    \n",
    "    ref_data['Bait_sumnorm'] = baitsumnorm\n",
    "    unique_preys = [p for p, v in preys_in_localizations.items() if len(v) == 1]\n",
    "    ref_data['Loc'] = [loc_data[loc_data['Bait']==bait].iloc[0][loc_col] for bait in ref_data['Bait'].values]\n",
    "    ref_data['Unique_to_loc'] = [prey in unique_preys for prey in ref_data['Prey'].values]\n",
    "\n",
    "    uref = ref_data[ref_data['Unique_to_loc']].copy()\n",
    "    locnorm = []\n",
    "    locsumnorm = []\n",
    "    loc_max = {}\n",
    "    loc_sum = {}\n",
    "    for l in uref['Loc'].unique():\n",
    "        loc_max[l] = uref[uref['Loc']==l]['AvgSpec'].max()\n",
    "        loc_sum[l] = uref[uref['Loc']==l]['AvgSpec'].sum()\n",
    "    for _,row in uref.iterrows():\n",
    "        locnorm.append(row['AvgSpec']/loc_max[row['Loc']])\n",
    "        locsumnorm.append(row['AvgSpec']/loc_sum[row['Loc']])\n",
    "    uref['Loc_norm'] = locnorm\n",
    "    uref['Loc_sumnorm'] = locsumnorm\n",
    "    uref['MSMIC_version'] = version\n",
    "    uref['Interaction'] = uref['Bait']+uref['Prey']\n",
    "\n",
    "    for _,row in uref.iterrows():\n",
    "        data = [\n",
    "            row[c.split()[0]] for c in msmictable_cols\n",
    "        ]\n",
    "        add_str = f'INSERT INTO msmicroscopy ({\", \".join([c.split()[0] for c in msmictable_cols])}) VALUES ({\", \".join([\"?\" for _ in msmictable_cols])})'\n",
    "        msmicroscopy_insert_sql.append([add_str, data])\n",
    "    print(version, len(msmicroscopy_insert_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad35c083-23f7-41cf-bd46-23048a67aaba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False: # This should no longer be needed\n",
    "    ms_inserts = []\n",
    "    other = []\n",
    "    have = set()\n",
    "    for insert_data in data_to_enter:\n",
    "        if insert_str.startswith('INSERT INTO ms_runs'):\n",
    "            # These two cause duplicates\n",
    "            if 'Trouble' in insert_data[3]:\n",
    "                continue\n",
    "            if 'BRE_20_xxxxx_Helsinki' in insert_data[3]:\n",
    "                continue\n",
    "            if insert_data[0] in have:continue # some duplicates ended up in the dataset\n",
    "            have.add(insert_data[0])\n",
    "            new_data = []\n",
    "            for i in insert_data:\n",
    "                if isinstance(i, list):\n",
    "                    new_data.extend(i)\n",
    "                else:\n",
    "                    new_data.append(i)\n",
    "            ms_inserts.append([insert_str, new_data])\n",
    "            ms +=1\n",
    "        else:\n",
    "            other.append([insert_str, insert_data])\n",
    "            o +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d96049ca-1b84-40f5-b1d8-a7d32c5729bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can proceed\n"
     ]
    }
   ],
   "source": [
    "print('can proceed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84a913cf-1d3e-481e-8ea4-426a804bfc4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10375\n"
     ]
    }
   ],
   "source": [
    "common_proteins_insert_sql = []\n",
    "comtable_create = ['CREATE TABLE IF NOT EXISTS common_proteins (']\n",
    "comdir = os.path.join(datadir,'Potential contaminant protein groups')\n",
    "com_cols = [\n",
    "    'uniprot_id TEXT PRIMARY KEY',\n",
    "    'gene_name TEXT',\n",
    "    'entry_name TEXT',\n",
    "    'all_gene_names TEXT',\n",
    "    'organism TEXT',\n",
    "    'protein_type TEXT NOT NULL',\n",
    "    'update_time TEXT NOT NULL '\n",
    "]\n",
    "for col in com_cols:\n",
    "    comtable_create.append(f'    {col},')\n",
    "comtable_create = '\\n'.join(comtable_create).strip(',')\n",
    "comtable_create += '\\n);'\n",
    "table_create_sql.append(comtable_create)\n",
    "common_proteins = {}\n",
    "\n",
    "for root, dirs, files in os.walk(comdir):\n",
    "    for f in files:\n",
    "        comdf = pd.read_csv(os.path.join(root,f),sep='\\t')\n",
    "        name = root.rsplit(os.sep,maxsplit=1)[-1]\n",
    "        for _,row in comdf.iterrows():\n",
    "            if row['Entry'] not in common_proteins:\n",
    "                common_proteins[row['Entry']] = [\n",
    "                    row['Entry'],\n",
    "                    row['Gene Names (primary)'],\n",
    "                    row['Entry Name'],\n",
    "                    row['Gene Names'],\n",
    "                    row['Organism'],\n",
    "                    [name],\n",
    "                    datetime.today().strftime('%Y-%m-%d')\n",
    "                ]\n",
    "            else:\n",
    "                common_proteins[row['Entry']][5].append(name)\n",
    "for common_protein, data in common_proteins.items():\n",
    "    data[5] = ', '.join(sorted(list(set(data[5]))))\n",
    "    add_str = f'INSERT INTO common_proteins ({\", \".join([c.split()[0] for c in com_cols])}) VALUES ({\", \".join([\"?\" for _ in com_cols])})'\n",
    "    common_proteins_insert_sql.append([add_str, data])\n",
    "print(len(common_proteins_insert_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6363e06-5d8c-447f-a1c0-059d3ce187fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table creation and data insertion took 18 seconds\n"
     ]
    }
   ],
   "source": [
    "# # Connect to the database (create it if it doesn't exist)\n",
    "conn = sqlite3.connect(os.path.join(dbdir,'proteogyver2.db'))\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "start = datetime.now()\n",
    "for create_table_str in table_create_sql:\n",
    "    if len(create_table_str) == 39:continue\n",
    "    cursor.execute(create_table_str)\n",
    "for insert_str, insert_data in control_insert_sql:\n",
    "    cursor.execute(insert_str, insert_data)\n",
    "for insert_str, insert_data in crapome_insert_sql:\n",
    "    cursor.execute(insert_str, insert_data)\n",
    "for insert_str, insert_data in proteins_insert_sql:\n",
    "    cursor.execute(insert_str, insert_data)\n",
    "for insert_str, insert_data in contaminants_insert_sql:\n",
    "    cursor.execute(insert_str, insert_data)\n",
    "for insert_str, insert_data in ms_runs_insert_sql:\n",
    "    cursor.execute(insert_str, insert_data)\n",
    "for insert_str, insert_data in known_interactions_insert_sql:\n",
    "    cursor.execute(insert_str, insert_data)\n",
    "for insert_str, insert_data in msmicroscopy_insert_sql:\n",
    "    cursor.execute(insert_str, insert_data)\n",
    "for insert_str, insert_data in common_proteins_insert_sql:\n",
    "    cursor.execute(insert_str, insert_data)\n",
    "print('Table creation and data insertion took', (datetime.now() - start).seconds, 'seconds')\n",
    "# Commit changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d47673a9-70fc-4340-93cd-852afa831072",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83898\n",
      "72577\n",
      "569793\n",
      "447\n",
      "12582\n",
      "2500023\n",
      "1603\n",
      "10375\n",
      "total 3251298\n"
     ]
    }
   ],
   "source": [
    "su = 0\n",
    "for iq in [\n",
    "    control_insert_sql,\n",
    "    crapome_insert_sql,\n",
    "    proteins_insert_sql,\n",
    "    contaminants_insert_sql,\n",
    "    ms_runs_insert_sql,\n",
    "    known_interactions_insert_sql,\n",
    "    msmicroscopy_insert_sql,\n",
    "    common_proteins_insert_sql\n",
    "]:\n",
    "    su += len(iq)\n",
    "    print(len(iq))\n",
    "print('total',su)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d305d6c-c730-4059-bffb-71475813fceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('control_sets',), ('crapome_sets',), ('proteins',), ('control_vl_gfp_mac3_10min_ap_overall',), ('control_vl_gfp_mac3_10min_ap',), ('control_vl_gfp_mac3_10min_bioid_overall',), ('control_vl_gfp_mac3_10min_bioid',), ('control_vl_gfp_mac2_18h_ap_overall',), ('control_vl_gfp_mac2_18h_ap',), ('control_vl_gfp_mac2_18h_bioid_overall',), ('control_vl_gfp_mac2_18h_bioid',), ('control_vl_gfp_mac_24h_ap_overall',), ('control_vl_gfp_mac_24h_ap',), ('control_vl_gfp_mac_24h_ap_nls_overall',), ('control_vl_gfp_mac_24h_ap_nls',), ('control_vl_gfp_mac_24h_bioid_overall',), ('control_vl_gfp_mac_24h_bioid',), ('control_vl_gfp_mac_24h_bioid_nls_overall',), ('control_vl_gfp_mac_24h_bioid_nls',), ('crapome_vl_gfp_mac3_10min_ap',), ('crapome_vl_gfp_mac3_10min_bioid',), ('crapome_vl_gfp_mac2_18h_ap',), ('crapome_vl_gfp_mac2_18h_bioid',), ('crapome_vl_gfp_mac_24h_ap',), ('crapome_vl_gfp_mac_24h_ap_nls',), ('crapome_vl_gfp_mac_24h_bioid',), ('crapome_vl_gfp_mac_24h_bioid_nls',), ('crapome_nesvilab',), ('contaminants',), ('ms_runs',), ('known_interactions',), ('msmicroscopy',), ('common_proteins',)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "con = sqlite3.connect(os.path.join(dbdir,'proteogyver.db'))\n",
    "cursor = con.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc7feb8-18ae-4de5-8f1b-2e92cabeab5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WIP_inprogress",
   "language": "python",
   "name": "wip_inprogress"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
